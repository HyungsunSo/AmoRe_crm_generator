{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85c9f2ec",
      "metadata": {
        "id": "85c9f2ec"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "코랩용 기존 파이프라인 vs. 파인튜닝된 파이프라인 비교 스크립트\n",
        "./adapters_dpo에 존재하는 어댑터를 기준으로 GPT-Score / 비GPT-Score를 비교.\n",
        "random_persona_campaign.csv의 더미 데이터를 기준으로 평가함.\n",
        "비교 문서는 adapter_comparison_{timestamp}.md로 저장.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c8e06164",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8e06164",
        "outputId": "f5fb2a85-2c0e-4521-dec0-a90b8d4fbb7b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e738c7ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e738c7ec",
        "outputId": "6365607b-b5a6-4a8f-c051-6890d8a17789"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.26.2)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.57.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.7.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Name: transformers\n",
            "Version: 4.57.3\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
            "Required-by: peft, sentence-transformers, trl\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets peft trl bitsandbytes accelerate\n",
        "!pip install -U transformers\n",
        "!pip show transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "42d8c26b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42d8c26b",
        "outputId": "2e5ba0d6-b520-4581-e027-6b42c8f8e2bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "716bd48b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "716bd48b",
        "outputId": "c2eac6a2-51a6-4418-eeb2-518f53d94704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "['.config', '.env', 'drive', '.ipynb_checkpoints', 'AmoRe_crm_generator', 'sample_data']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f903104c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f903104c",
        "outputId": "935c7226-ccd6-4fc4-c292-3925fcbb3fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AmoRe_crm_generator/finetuning\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/jjjh02/AmoRe_crm_generator.git\n",
        "# %cd AmoRe_crm_generator\n",
        "# !git checkout jinhyeok\n",
        "# !git branch\n",
        "os.chdir(\"/content/AmoRe_crm_generator/finetuning\")\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7zWvPPsa673",
        "outputId": "7bfc2722-73e3-4c80-9ba9-8688ce602bcb"
      },
      "id": "R7zWvPPsa673",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f1e4a51",
      "metadata": {
        "id": "6f1e4a51"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import argparse\n",
        "import csv\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import urllib.error\n",
        "import urllib.request\n",
        "from collections import Counter\n",
        "from contextlib import contextmanager\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "\n",
        "BASE_DIR = os.getcwd()\n",
        "SRC_DIR = os.path.abspath(os.path.join(BASE_DIR, \"..\", \"src\"))\n",
        "DEFAULT_CSV = os.path.join(BASE_DIR, \"random_persona_campaign.csv\")\n",
        "DEFAULT_ADAPTER_DIR = \"/content/drive/MyDrive/멋사/adapters_dpo_2\"\n",
        "STAGE_ORDER = [\"Acquisition\", \"Activation\", \"Retention\", \"Revenue\", \"Referral\"]\n",
        "out_path = \"/content/drive/MyDrive/멋사/comparison_dpo/comparison_01\"\n",
        "\n",
        "print(BASE_DIR, SRC_DIR)\n",
        "def _log(message):\n",
        "    print(message)\n",
        "\n",
        "\n",
        "def _import_pipeline_module():\n",
        "    if SRC_DIR not in sys.path:\n",
        "        sys.path.insert(0, SRC_DIR)\n",
        "    try:\n",
        "        import run_qwen_exaone_pipeline as pipeline_module\n",
        "    except Exception as exc:\n",
        "        raise ImportError(\n",
        "            \"Failed to import main from ../src/run_qwen_exaone_pipeline.py\"\n",
        "        ) from exc\n",
        "    return pipeline_module\n",
        "\n",
        "\n",
        "def _load_json(path):\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        return None\n",
        "\n",
        "\n",
        "def _parse_bool(value):\n",
        "    if isinstance(value, bool):\n",
        "        return value\n",
        "    if value is None:\n",
        "        return False\n",
        "    if isinstance(value, (int, float)):\n",
        "        return bool(value)\n",
        "    text = str(value).strip().lower()\n",
        "    return text in {\"1\", \"true\", \"yes\", \"y\", \"t\"}\n",
        "\n",
        "\n",
        "def _load_rows(csv_path):\n",
        "    with open(csv_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            if not row:\n",
        "                continue\n",
        "            persona_raw = row.get(\"persona\", \"\").strip()\n",
        "            brand_raw = row.get(\"brand\", \"\").strip()\n",
        "            product_raw = row.get(\"product\", \"\").strip()\n",
        "            stage_raw = row.get(\"stage_index\", \"\").strip()\n",
        "            style_raw = row.get(\"style_index\", \"\").strip()\n",
        "            if not persona_raw or not brand_raw or not product_raw:\n",
        "                continue\n",
        "            if not stage_raw or not style_raw:\n",
        "                continue\n",
        "            try:\n",
        "                persona = int(persona_raw)\n",
        "                stage_index = int(stage_raw)\n",
        "                style_index = int(style_raw)\n",
        "            except ValueError:\n",
        "                continue\n",
        "            yield {\n",
        "                \"persona\": persona,\n",
        "                \"brand\": brand_raw,\n",
        "                \"product\": product_raw,\n",
        "                \"stage_index\": stage_index,\n",
        "                \"style_index\": style_index,\n",
        "                \"is_event\": _parse_bool(row.get(\"is_event\", \"\")),\n",
        "            }\n",
        "\n",
        "\n",
        "def _get_stage_name(stage_index):\n",
        "    if isinstance(stage_index, int) and 0 <= stage_index < len(STAGE_ORDER):\n",
        "        return STAGE_ORDER[stage_index]\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def _get_crm_goal(crm_goals, stage_index, stage_name=None):\n",
        "    if not isinstance(crm_goals, dict):\n",
        "        return {}\n",
        "    if stage_name and stage_name in crm_goals:\n",
        "        return crm_goals.get(stage_name, {}) or {}\n",
        "    stage_name = _get_stage_name(stage_index)\n",
        "    if stage_name:\n",
        "        return crm_goals.get(stage_name, {}) or {}\n",
        "    return {}\n",
        "\n",
        "\n",
        "def _get_brand_story(brand_stories, brand_name):\n",
        "    if not isinstance(brand_stories, dict) or not brand_name:\n",
        "        return {}\n",
        "    if brand_name in brand_stories:\n",
        "        return brand_stories.get(brand_name, {}) or {}\n",
        "    for story in brand_stories.values():\n",
        "        if str(story.get(\"name_en\", \"\")).lower() == brand_name.lower():\n",
        "            return story\n",
        "    return {}\n",
        "\n",
        "\n",
        "def _format_event(selected_event):\n",
        "    if selected_event in (None, \"\", {}):\n",
        "        return \"none\"\n",
        "    if isinstance(selected_event, dict):\n",
        "        for key in (\"title\", \"name\", \"event_name\", \"event\"):\n",
        "            if selected_event.get(key):\n",
        "                return str(selected_event.get(key))\n",
        "        return json.dumps(selected_event, ensure_ascii=False)\n",
        "    return str(selected_event)\n",
        "\n",
        "\n",
        "def _format_price(price):\n",
        "    if price in (None, \"\"):\n",
        "        return \"\"\n",
        "    if isinstance(price, (int, float)):\n",
        "        return f\"{int(price):,} KRW\"\n",
        "    text = str(price).strip()\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    if text.replace(\",\", \"\").isdigit():\n",
        "        return f\"{int(text.replace(',', '')):,} KRW\"\n",
        "    return text\n",
        "\n",
        "\n",
        "def _format_persona(persona_profile):\n",
        "    if not isinstance(persona_profile, dict):\n",
        "        return str(persona_profile or \"\")\n",
        "    name = persona_profile.get(\"name\", \"\")\n",
        "    extras = []\n",
        "    value_focus = persona_profile.get(\"value_focus\")\n",
        "    skin_type = persona_profile.get(\"skin_type\")\n",
        "    traits = persona_profile.get(\"traits\")\n",
        "    shopping_style = persona_profile.get(\"shopping_style\")\n",
        "    if value_focus:\n",
        "        extras.append(str(value_focus))\n",
        "    if skin_type:\n",
        "        extras.append(str(skin_type))\n",
        "    if traits:\n",
        "        if isinstance(traits, list):\n",
        "            extras.append(\", \".join([str(t) for t in traits if t]))\n",
        "        else:\n",
        "            extras.append(str(traits))\n",
        "    if shopping_style:\n",
        "        extras.append(str(shopping_style))\n",
        "    extra_text = \", \".join([e for e in extras if e])\n",
        "    if name and extra_text:\n",
        "        return f\"{name} ({extra_text})\"\n",
        "    return name or extra_text\n",
        "\n",
        "\n",
        "def _build_context_block(out, max_style_templates=3):\n",
        "    persona = _format_persona(out.get(\"persona_profile\"))\n",
        "    stage = out.get(\"stage_name\") or out.get(\"stage_kr\") or \"\"\n",
        "    brand = out.get(\"brand\") or \"\"\n",
        "    product_basic = out.get(\"product_basic\") if isinstance(out.get(\"product_basic\"), dict) else {}\n",
        "    product_name = product_basic.get(\"name\") or out.get(\"product_query\") or \"\"\n",
        "    price = _format_price(product_basic.get(\"price\"))\n",
        "    objective = out.get(\"objective\") or \"\"\n",
        "    target_state = out.get(\"target_state\") or \"\"\n",
        "    style_templates = out.get(\"style_templates\") or []\n",
        "    if isinstance(style_templates, list):\n",
        "        style_templates = style_templates[:max_style_templates]\n",
        "    selected_event = _format_event(out.get(\"selected_event\"))\n",
        "\n",
        "    lines = [\"[Context]\"]\n",
        "    if persona:\n",
        "        lines.append(f\"- Persona: {persona}\")\n",
        "    if stage:\n",
        "        lines.append(f\"- Stage: {stage}\")\n",
        "    if brand or product_name:\n",
        "        lines.append(f\"- Brand/Product: {brand} / {product_name}\".strip())\n",
        "    if price:\n",
        "        lines.append(f\"- Price: {price}\")\n",
        "    if objective:\n",
        "        lines.append(f\"- Objective: {objective}\")\n",
        "    if target_state:\n",
        "        lines.append(f\"- Target state: {target_state}\")\n",
        "    if style_templates:\n",
        "        lines.append(\"- Style templates:\")\n",
        "        for item in style_templates:\n",
        "            lines.append(f\"  - {item}\")\n",
        "    lines.append(f\"- Event: {selected_event}\")\n",
        "    return \"\\n\".join(lines).strip()\n",
        "\n",
        "\n",
        "def _extract_message(out):\n",
        "    exaone = out.get(\"exaone\", {}) if isinstance(out, dict) else {}\n",
        "    return exaone.get(\"result_raw\") or \"\"\n",
        "\n",
        "\n",
        "def _tokenize(text):\n",
        "    if not text:\n",
        "        return []\n",
        "    return [t for t in re.split(r\"\\s+\", str(text)) if len(t) > 1]\n",
        "\n",
        "\n",
        "def _split_tokens(text):\n",
        "    if not text:\n",
        "        return []\n",
        "    cleaned = re.sub(r\"[^\\w\\uac00-\\ud7a3]+\", \" \", str(text), flags=re.UNICODE)\n",
        "    return [t for t in cleaned.split() if len(t) > 1]\n",
        "\n",
        "\n",
        "def _extract_keywords(texts, max_terms=30):\n",
        "    counter = Counter()\n",
        "    for text in texts:\n",
        "        for token in _split_tokens(text):\n",
        "            if token.isdigit():\n",
        "                continue\n",
        "            counter[token] += 1\n",
        "    if not counter:\n",
        "        return []\n",
        "    return [item for item, _ in counter.most_common(max_terms)]\n",
        "\n",
        "\n",
        "def _coverage_score(message, out):\n",
        "    total = 0\n",
        "    hits = 0\n",
        "    if not message:\n",
        "        return 0.0\n",
        "\n",
        "    brand = out.get(\"brand\")\n",
        "    if brand:\n",
        "        total += 1\n",
        "        if brand in message:\n",
        "            hits += 1\n",
        "\n",
        "    product_basic = out.get(\"product_basic\") if isinstance(out.get(\"product_basic\"), dict) else {}\n",
        "    product_name = product_basic.get(\"name\") or out.get(\"product_query\") or \"\"\n",
        "    if product_name:\n",
        "        total += 1\n",
        "        if product_name in message:\n",
        "            hits += 1\n",
        "\n",
        "    selected_event = _format_event(out.get(\"selected_event\"))\n",
        "    if selected_event and selected_event != \"none\":\n",
        "        total += 1\n",
        "        if selected_event in message:\n",
        "            hits += 1\n",
        "\n",
        "    stage_terms = []\n",
        "    for text in (out.get(\"stage_kr\"), out.get(\"objective\"), out.get(\"target_state\")):\n",
        "        stage_terms.extend(_tokenize(text))\n",
        "    if stage_terms:\n",
        "        total += 1\n",
        "        if any(term in message for term in stage_terms):\n",
        "            hits += 1\n",
        "\n",
        "    return hits / total if total else 0.0\n",
        "\n",
        "\n",
        "def _tone_match_score(message, brand_story):\n",
        "    if not message or not isinstance(brand_story, dict):\n",
        "        return 0.0\n",
        "    tone_keywords = brand_story.get(\"tone_keywords\") or []\n",
        "    if not tone_keywords:\n",
        "        return 0.0\n",
        "    hits = sum(1 for kw in tone_keywords if kw and kw in message)\n",
        "    return hits / len(tone_keywords)\n",
        "\n",
        "\n",
        "def _style_match_score(message, style_templates, max_terms=30):\n",
        "    if not message or not style_templates:\n",
        "        return 0.0\n",
        "    if not isinstance(style_templates, list):\n",
        "        style_templates = [str(style_templates)]\n",
        "    keywords = _extract_keywords(style_templates, max_terms=max_terms)\n",
        "    if not keywords:\n",
        "        return 0.0\n",
        "    hits = sum(1 for kw in keywords if kw in message)\n",
        "    return hits / len(keywords)\n",
        "\n",
        "\n",
        "def _info_density(message, out):\n",
        "    if not message:\n",
        "        return 0.0\n",
        "    persona = out.get(\"persona_profile\") if isinstance(out.get(\"persona_profile\"), dict) else {}\n",
        "    product_basic = out.get(\"product_basic\") if isinstance(out.get(\"product_basic\"), dict) else {}\n",
        "    context_texts = [\n",
        "        out.get(\"brand\"),\n",
        "        product_basic.get(\"name\"),\n",
        "        out.get(\"product_query\"),\n",
        "        out.get(\"stage_kr\"),\n",
        "        out.get(\"objective\"),\n",
        "        out.get(\"target_state\"),\n",
        "        persona.get(\"value_focus\"),\n",
        "        persona.get(\"skin_type\"),\n",
        "    ]\n",
        "    if isinstance(persona.get(\"traits\"), list):\n",
        "        context_texts.extend(persona.get(\"traits\"))\n",
        "    if persona.get(\"shopping_style\"):\n",
        "        context_texts.append(persona.get(\"shopping_style\"))\n",
        "\n",
        "    keywords = _extract_keywords([t for t in context_texts if t], max_terms=40)\n",
        "    if not keywords:\n",
        "        return 0.0\n",
        "    message_tokens = _split_tokens(message)\n",
        "    if not message_tokens:\n",
        "        return 0.0\n",
        "    hits = sum(1 for kw in keywords if kw in message)\n",
        "    return hits / len(message_tokens)\n",
        "\n",
        "\n",
        "def _repetition_stats(message):\n",
        "    tokens = _split_tokens(message)\n",
        "    if not tokens:\n",
        "        return 0.0, 0.0\n",
        "    unique_tokens = set(tokens)\n",
        "    repeat_token_ratio = (len(tokens) - len(unique_tokens)) / len(tokens)\n",
        "\n",
        "    if len(tokens) < 6:\n",
        "        return repeat_token_ratio, 0.0\n",
        "    n = 3\n",
        "    ngrams = [\" \".join(tokens[i:i + n]) for i in range(len(tokens) - n + 1)]\n",
        "    counts = Counter(ngrams)\n",
        "    total_ngrams = len(ngrams)\n",
        "    repeated = sum(count - 1 for count in counts.values() if count > 1)\n",
        "    repeat_ngram_ratio = repeated / total_ngrams if total_ngrams else 0.0\n",
        "    return repeat_token_ratio, repeat_ngram_ratio\n",
        "\n",
        "\n",
        "def _length_target(stage_name):\n",
        "    if stage_name == \"Acquisition\":\n",
        "        return 60, 200\n",
        "    if stage_name == \"Activation\":\n",
        "        return 60, 200\n",
        "    if stage_name == \"Retention\":\n",
        "        return 60, 180\n",
        "    if stage_name == \"Revenue\":\n",
        "        return 60, 180\n",
        "    if stage_name == \"Referral\":\n",
        "        return 60, 160\n",
        "    return 50, 220\n",
        "\n",
        "\n",
        "def _length_ok(message, stage_name):\n",
        "    if not message:\n",
        "        return False\n",
        "    min_len, max_len = _length_target(stage_name)\n",
        "    return min_len <= len(message) <= max_len\n",
        "\n",
        "\n",
        "def _forbidden_violations(message, crm_goal):\n",
        "    if not message or not isinstance(crm_goal, dict):\n",
        "        return 0\n",
        "    forbidden = crm_goal.get(\"forbidden_context\") or []\n",
        "    if not forbidden:\n",
        "        return 0\n",
        "    hits = 0\n",
        "    for term in forbidden:\n",
        "        if term and term in message:\n",
        "            hits += 1\n",
        "    return hits\n",
        "\n",
        "\n",
        "def _cta_present(message):\n",
        "    if not message:\n",
        "        return False\n",
        "    cta_markers = [\n",
        "        \"\\uc9c0\\uae08\", \"\\ud655\\uc778\", \"\\uad6c\\ub9e4\", \"\\uc2e0\\uccad\", \"\\ucc38\\uc5ec\",\n",
        "        \"\\ud074\\ub9ad\", \"\\ubc1b\\uae30\", \"\\ud61c\\ud0dd\", \"\\ud560\\uc778\", \"\\ucfe0\\ud3f0\",\n",
        "        \"\\ud574\\ubcf4\\uc138\\uc694\", \"\\ud558\\uc138\\uc694\", \"\\ub458\\ub7ec\\ubcf4\\uae30\",\n",
        "        \"\\ubc14\\ub85c\", \"\\ucd94\\ucc9c\", \"\\ubb38\\uc758\"\n",
        "    ]\n",
        "    return any(marker in message for marker in cta_markers)\n",
        "\n",
        "\n",
        "def _call_gpt(context_block, base_message, adapter_message):\n",
        "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise RuntimeError(\"OPENAI_API_KEY is not set.\")\n",
        "\n",
        "    candidate_block = (\n",
        "        \"[0]\\n\"\n",
        "        f\"{base_message}\\n\\n\"\n",
        "        \"[1]\\n\"\n",
        "        f\"{adapter_message}\"\n",
        "    )\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are evaluating CRM messages. Pick the message more likely to drive conversion.\\n\"\n",
        "        \"Compare with these criteria:\\n\"\n",
        "        \"1) Likelihood of action (click/repurchase)\\n\"\n",
        "        \"2) Fit to persona and stage goals\\n\"\n",
        "        \"3) Brand/product value delivery\\n\"\n",
        "        \"4) Use of style templates and event context when applicable\\n\"\n",
        "        \"5) Clarity without unnecessary decoration\\n\"\n",
        "        \"Return only the best candidate index as an integer (0 or 1).\"\n",
        "    )\n",
        "    user_prompt = (\n",
        "        \"Context:\\n\"\n",
        "        f\"{context_block}\\n\\n\"\n",
        "        \"Candidates:\\n\"\n",
        "        f\"{candidate_block}\\n\\n\"\n",
        "        \"Return only the best candidate index.\"\n",
        "    )\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"gpt-5-nano\",\n",
        "        \"input\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": [{\"type\": \"input_text\", \"text\": system_prompt}],\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [{\"type\": \"input_text\", \"text\": user_prompt}],\n",
        "            },\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    request = urllib.request.Request(\n",
        "        \"https://api.openai.com/v1/responses\",\n",
        "        data=json.dumps(payload).encode(\"utf-8\"),\n",
        "        headers={\n",
        "            \"Authorization\": f\"Bearer {api_key}\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "        },\n",
        "        method=\"POST\",\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        with urllib.request.urlopen(request, timeout=30) as response:\n",
        "            data = json.loads(response.read().decode(\"utf-8\"))\n",
        "    except urllib.error.HTTPError as exc:\n",
        "        body = exc.read().decode(\"utf-8\", errors=\"replace\")\n",
        "        raise RuntimeError(f\"OpenAI API error {exc.code}: {body}\") from exc\n",
        "\n",
        "    output_text = _extract_response_text(data)\n",
        "    match = re.search(r\"-?\\d+\", str(output_text))\n",
        "    if not match:\n",
        "        raise ValueError(f\"Invalid evaluator response: {output_text}\")\n",
        "    choice = int(match.group(0))\n",
        "    if choice not in (0, 1):\n",
        "        raise ValueError(f\"Evaluator index out of range: {choice}\")\n",
        "    return choice\n",
        "\n",
        "def _extract_response_text(data):\n",
        "    if isinstance(data, dict):\n",
        "        output_text = data.get(\"output_text\")\n",
        "        if isinstance(output_text, str) and output_text.strip():\n",
        "            return output_text.strip()\n",
        "\n",
        "        output = data.get(\"output\")\n",
        "        if isinstance(output, list):\n",
        "            parts = []\n",
        "            for item in output:\n",
        "                if not isinstance(item, dict):\n",
        "                    continue\n",
        "                content = item.get(\"content\", [])\n",
        "                if isinstance(content, list):\n",
        "                    for block in content:\n",
        "                        if isinstance(block, dict) and isinstance(block.get(\"text\"), str):\n",
        "                            parts.append(block[\"text\"])\n",
        "                        elif isinstance(block, str):\n",
        "                            parts.append(block)\n",
        "                elif isinstance(content, str):\n",
        "                    parts.append(content)\n",
        "            if parts:\n",
        "                return \"\".join(parts).strip()\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "@contextmanager\n",
        "def _patch_exaone(pipeline_module, adapter_path=None):\n",
        "    import tone_correction\n",
        "\n",
        "    class PatchedExaoneToneCorrector(tone_correction.ExaoneToneCorrector):\n",
        "        _cache = {}\n",
        "\n",
        "        def __init__(self, model_name=\"LGAI-EXAONE/EXAONE-4.0-1.2B\"):\n",
        "            key = (model_name, adapter_path)\n",
        "            cached = self._cache.get(key)\n",
        "            if cached:\n",
        "                self.device = cached[\"device\"]\n",
        "                self.model_name = model_name\n",
        "                self.tokenizer = cached[\"tokenizer\"]\n",
        "                self.model = cached[\"model\"]\n",
        "                return\n",
        "            super().__init__(model_name=model_name)\n",
        "            if adapter_path:\n",
        "                try:\n",
        "                    from peft import PeftModel\n",
        "                except ImportError as exc:\n",
        "                    raise RuntimeError(\"peft is required to load adapters.\") from exc\n",
        "                self.model = PeftModel.from_pretrained(self.model, adapter_path)\n",
        "                try:\n",
        "                    self.model.eval()\n",
        "                except Exception:\n",
        "                    pass\n",
        "            self._cache[key] = {\n",
        "                \"device\": self.device,\n",
        "                \"tokenizer\": self.tokenizer,\n",
        "                \"model\": self.model,\n",
        "            }\n",
        "\n",
        "    original = pipeline_module.ExaoneToneCorrector\n",
        "    pipeline_module.ExaoneToneCorrector = PatchedExaoneToneCorrector\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        pipeline_module.ExaoneToneCorrector = original\n",
        "\n",
        "\n",
        "def _run_pipeline_main(pipeline_main, row):\n",
        "    argv = [\n",
        "        \"run_qwen_exaone_pipeline.py\",\n",
        "        \"--persona\",\n",
        "        str(row[\"persona\"]),\n",
        "        \"--brand\",\n",
        "        row[\"brand\"],\n",
        "        \"--product\",\n",
        "        row[\"product\"],\n",
        "        \"--stage_index\",\n",
        "        str(row[\"stage_index\"]),\n",
        "        \"--style_index\",\n",
        "        str(row[\"style_index\"]),\n",
        "        \"--is_event\",\n",
        "        \"1\" if row.get(\"is_event\", False) else \"0\",\n",
        "    ]\n",
        "    old_argv = sys.argv\n",
        "    try:\n",
        "        sys.argv = argv\n",
        "        return pipeline_main()\n",
        "    finally:\n",
        "        sys.argv = old_argv\n",
        "\n",
        "\n",
        "def _write_report(out_path, summary, rows, max_examples):\n",
        "    lines = []\n",
        "    lines.append(\"# Adapter Comparison Report\")\n",
        "    lines.append(\"\")\n",
        "    lines.append(f\"- CSV: {summary['csv']}\")\n",
        "    lines.append(f\"- Adapter: {summary['adapter']}\")\n",
        "    lines.append(f\"- Samples: {summary['samples']}\")\n",
        "    lines.append(\"\")\n",
        "    lines.append(\"## Summary\")\n",
        "    lines.append(\"\")\n",
        "    for item in summary[\"metrics\"]:\n",
        "        lines.append(f\"- {item}\")\n",
        "    lines.append(\"\")\n",
        "    lines.append(\"## Per-sample Results\")\n",
        "    lines.append(\"\")\n",
        "    lines.append(\n",
        "        \"| idx | persona | brand/product | stage | event | gpt winner | base len | adapter len | base cov | adapter cov | base tone | adapter tone | base style | adapter style | base dens | adapter dens | base rep tok | adapter rep tok | base rep 3g | adapter rep 3g | base len ok | adapter len ok | base forb | adapter forb | base cta | adapter cta |\"\n",
        "    )\n",
        "    lines.append(\n",
        "        \"| --- | --- | --- | --- | --- | --- | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | --- | --- | ---: | ---: | --- | --- |\"\n",
        "    )\n",
        "    for item in rows:\n",
        "        lines.append(\n",
        "            \"| {idx} | {persona} | {brand_product} | {stage} | {event} | {winner} | {base_len} | {adapter_len} | {base_cov:.2f} | {adapter_cov:.2f} | {base_tone:.2f} | {adapter_tone:.2f} | {base_style:.2f} | {adapter_style:.2f} | {base_density:.2f} | {adapter_density:.2f} | {base_rep_token:.2f} | {adapter_rep_token:.2f} | {base_rep_ngram:.2f} | {adapter_rep_ngram:.2f} | {base_len_ok} | {adapter_len_ok} | {base_forbidden} | {adapter_forbidden} | {base_cta} | {adapter_cta} |\".format(\n",
        "                idx=item[\"idx\"],\n",
        "                persona=item[\"persona\"],\n",
        "                brand_product=item[\"brand_product\"],\n",
        "                stage=item[\"stage\"],\n",
        "                event=item[\"event\"],\n",
        "                winner=item[\"winner\"],\n",
        "                base_len=item[\"base_len\"],\n",
        "                adapter_len=item[\"adapter_len\"],\n",
        "                base_cov=item[\"base_cov\"],\n",
        "                adapter_cov=item[\"adapter_cov\"],\n",
        "                base_tone=item[\"base_tone\"],\n",
        "                adapter_tone=item[\"adapter_tone\"],\n",
        "                base_style=item[\"base_style\"],\n",
        "                adapter_style=item[\"adapter_style\"],\n",
        "                base_density=item[\"base_density\"],\n",
        "                adapter_density=item[\"adapter_density\"],\n",
        "                base_rep_token=item[\"base_rep_token\"],\n",
        "                adapter_rep_token=item[\"adapter_rep_token\"],\n",
        "                base_rep_ngram=item[\"base_rep_ngram\"],\n",
        "                adapter_rep_ngram=item[\"adapter_rep_ngram\"],\n",
        "                base_len_ok=\"yes\" if item[\"base_len_ok\"] else \"no\",\n",
        "                adapter_len_ok=\"yes\" if item[\"adapter_len_ok\"] else \"no\",\n",
        "                base_forbidden=item[\"base_forbidden\"],\n",
        "                adapter_forbidden=item[\"adapter_forbidden\"],\n",
        "                base_cta=\"yes\" if item[\"base_cta\"] else \"no\",\n",
        "                adapter_cta=\"yes\" if item[\"adapter_cta\"] else \"no\",\n",
        "            )\n",
        "        )\n",
        "    lines.append(\"\")\n",
        "\n",
        "    if max_examples > 0:\n",
        "        lines.append(\"## Examples\")\n",
        "        lines.append(\"\")\n",
        "        for item in rows[:max_examples]:\n",
        "            lines.append(f\"### Sample {item['idx']}\")\n",
        "            lines.append(\"\")\n",
        "            lines.append(\"Context:\")\n",
        "            lines.append(\"\")\n",
        "            lines.append(\"```\")\n",
        "            lines.append(item[\"context\"])\n",
        "            lines.append(\"```\")\n",
        "            lines.append(\"\")\n",
        "            lines.append(\"Base message:\")\n",
        "            lines.append(\"\")\n",
        "            lines.append(\"```\")\n",
        "            lines.append(item[\"base_message\"])\n",
        "            lines.append(\"```\")\n",
        "            lines.append(\"\")\n",
        "            lines.append(\"Adapter message:\")\n",
        "            lines.append(\"\")\n",
        "            lines.append(\"```\")\n",
        "            lines.append(item[\"adapter_message\"])\n",
        "            lines.append(\"```\")\n",
        "            lines.append(\"\")\n",
        "            lines.append(f\"GPT winner: {item['winner']}\")\n",
        "            lines.append(\"\")\n",
        "\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        print([line for line in lines])\n",
        "        f.write(\"\\n\".join(lines))\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--csv_path\", default=DEFAULT_CSV)\n",
        "parser.add_argument(\"--adapter_path\", default=DEFAULT_ADAPTER_DIR)\n",
        "parser.add_argument(\"--out_path\", default=\"content/drive/MyDrive/멋사/comparison_dpo\")\n",
        "parser.add_argument(\"--max_rows\", type=int, default=10)\n",
        "parser.add_argument(\"--max_examples\", type=int, default=3)\n",
        "parser.add_argument(\"--skip_llm_eval\", action=\"store_true\")\n",
        "parser.add_argument(\"--max_style_templates\", type=int, default=3)\n",
        "args, _ = parser.parse_known_args()\n",
        "\n",
        "if not os.path.exists(args.csv_path):\n",
        "    raise FileNotFoundError(f\"CSV not found: {args.csv_path}\")\n",
        "if not os.path.exists(args.adapter_path):\n",
        "    raise FileNotFoundError(f\"Adapter not found: {args.adapter_path}\")\n",
        "\n",
        "pipeline_module = _import_pipeline_module()\n",
        "pipeline_main = pipeline_module.main\n",
        "brand_stories = _load_json(os.path.join(BASE_DIR, \"data\", \"brand_stories.json\"))\n",
        "crm_goals = _load_json(os.path.join(BASE_DIR, \"data\", \"crm_goals.json\"))\n",
        "\n",
        "rows = []\n",
        "for idx, row in enumerate(_load_rows(args.csv_path)):\n",
        "    if args.max_rows is not None and idx >= args.max_rows:\n",
        "        break\n",
        "    rows.append(row)\n",
        "\n",
        "if not rows:\n",
        "    raise RuntimeError(\"No rows to evaluate.\")\n",
        "\n",
        "results = []\n",
        "wins = {\"base\": 0, \"adapter\": 0}\n",
        "\n",
        "for idx, row in enumerate(rows, start=1):\n",
        "    _log(\n",
        "        \"[Row {idx}] persona={persona} brand={brand} product={product} \"\n",
        "        \"stage_index={stage_index} style_index={style_index} is_event={is_event}\".format(\n",
        "            idx=idx,\n",
        "            persona=row[\"persona\"],\n",
        "            brand=row[\"brand\"],\n",
        "            product=row[\"product\"],\n",
        "            stage_index=row[\"stage_index\"],\n",
        "            style_index=row[\"style_index\"],\n",
        "            is_event=row.get(\"is_event\", False),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    _log(\"  Running base pipeline...\")\n",
        "    base_out = _run_pipeline_main(pipeline_main, row)\n",
        "\n",
        "    _log(\"  Running adapter pipeline...\")\n",
        "    with _patch_exaone(pipeline_module, adapter_path=args.adapter_path):\n",
        "        adapter_out = _run_pipeline_main(pipeline_main, row)\n",
        "\n",
        "    base_message = _extract_message(base_out)\n",
        "    adapter_message = _extract_message(adapter_out)\n",
        "\n",
        "    context_block = _build_context_block(base_out, args.max_style_templates)\n",
        "    stage_name = base_out.get(\"stage_name\") or _get_stage_name(row[\"stage_index\"])\n",
        "    crm_goal = _get_crm_goal(crm_goals, row[\"stage_index\"], stage_name)\n",
        "    brand_story = _get_brand_story(brand_stories, base_out.get(\"brand\"))\n",
        "\n",
        "    winner = \"n/a\"\n",
        "    if not args.skip_llm_eval:\n",
        "        choice = _call_gpt(context_block, base_message, adapter_message)\n",
        "        winner = \"base\" if choice == 0 else \"adapter\"\n",
        "        wins[winner] += 1\n",
        "\n",
        "    base_cov = _coverage_score(base_message, base_out)\n",
        "    adapter_cov = _coverage_score(adapter_message, base_out)\n",
        "    base_tone = _tone_match_score(base_message, brand_story)\n",
        "    adapter_tone = _tone_match_score(adapter_message, brand_story)\n",
        "    base_style = _style_match_score(base_message, base_out.get(\"style_templates\"))\n",
        "    adapter_style = _style_match_score(adapter_message, base_out.get(\"style_templates\"))\n",
        "    base_density = _info_density(base_message, base_out)\n",
        "    adapter_density = _info_density(adapter_message, base_out)\n",
        "    base_rep_token, base_rep_ngram = _repetition_stats(base_message)\n",
        "    adapter_rep_token, adapter_rep_ngram = _repetition_stats(adapter_message)\n",
        "    base_len_ok = _length_ok(base_message, stage_name)\n",
        "    adapter_len_ok = _length_ok(adapter_message, stage_name)\n",
        "    base_forbidden = _forbidden_violations(base_message, crm_goal)\n",
        "    adapter_forbidden = _forbidden_violations(adapter_message, crm_goal)\n",
        "    base_len = len(base_message)\n",
        "    adapter_len = len(adapter_message)\n",
        "\n",
        "    results.append(\n",
        "        {\n",
        "            \"idx\": idx,\n",
        "            \"persona\": row[\"persona\"],\n",
        "            \"brand_product\": f\"{row['brand']} / {row['product']}\",\n",
        "            \"stage\": base_out.get(\"stage_name\") or base_out.get(\"stage_kr\") or \"\",\n",
        "            \"event\": _format_event(base_out.get(\"selected_event\")),\n",
        "            \"winner\": winner,\n",
        "            \"base_len\": base_len,\n",
        "            \"adapter_len\": adapter_len,\n",
        "            \"base_cov\": base_cov,\n",
        "            \"adapter_cov\": adapter_cov,\n",
        "            \"base_tone\": base_tone,\n",
        "            \"adapter_tone\": adapter_tone,\n",
        "            \"base_style\": base_style,\n",
        "            \"adapter_style\": adapter_style,\n",
        "            \"base_density\": base_density,\n",
        "            \"adapter_density\": adapter_density,\n",
        "            \"base_rep_token\": base_rep_token,\n",
        "            \"adapter_rep_token\": adapter_rep_token,\n",
        "            \"base_rep_ngram\": base_rep_ngram,\n",
        "            \"adapter_rep_ngram\": adapter_rep_ngram,\n",
        "            \"base_len_ok\": base_len_ok,\n",
        "            \"adapter_len_ok\": adapter_len_ok,\n",
        "            \"base_forbidden\": base_forbidden,\n",
        "            \"adapter_forbidden\": adapter_forbidden,\n",
        "            \"base_cta\": _cta_present(base_message),\n",
        "            \"adapter_cta\": _cta_present(adapter_message),\n",
        "            \"context\": context_block,\n",
        "            \"base_message\": base_message,\n",
        "            \"adapter_message\": adapter_message,\n",
        "        }\n",
        "    )\n",
        "\n",
        "avg_base_cov = sum(r[\"base_cov\"] for r in results) / len(results)\n",
        "avg_adapter_cov = sum(r[\"adapter_cov\"] for r in results) / len(results)\n",
        "avg_base_tone = sum(r[\"base_tone\"] for r in results) / len(results)\n",
        "avg_adapter_tone = sum(r[\"adapter_tone\"] for r in results) / len(results)\n",
        "avg_base_style = sum(r[\"base_style\"] for r in results) / len(results)\n",
        "avg_adapter_style = sum(r[\"adapter_style\"] for r in results) / len(results)\n",
        "avg_base_density = sum(r[\"base_density\"] for r in results) / len(results)\n",
        "avg_adapter_density = sum(r[\"adapter_density\"] for r in results) / len(results)\n",
        "avg_base_rep_token = sum(r[\"base_rep_token\"] for r in results) / len(results)\n",
        "avg_adapter_rep_token = sum(r[\"adapter_rep_token\"] for r in results) / len(results)\n",
        "avg_base_rep_ngram = sum(r[\"base_rep_ngram\"] for r in results) / len(results)\n",
        "avg_adapter_rep_ngram = sum(r[\"adapter_rep_ngram\"] for r in results) / len(results)\n",
        "base_len_ok_rate = sum(1 for r in results if r[\"base_len_ok\"]) / len(results)\n",
        "adapter_len_ok_rate = sum(1 for r in results if r[\"adapter_len_ok\"]) / len(results)\n",
        "base_forbidden_rate = sum(1 for r in results if r[\"base_forbidden\"] > 0) / len(results)\n",
        "adapter_forbidden_rate = sum(1 for r in results if r[\"adapter_forbidden\"] > 0) / len(results)\n",
        "base_cta_rate = sum(1 for r in results if r[\"base_cta\"]) / len(results)\n",
        "adapter_cta_rate = sum(1 for r in results if r[\"adapter_cta\"]) / len(results)\n",
        "avg_base_len = sum(r[\"base_len\"] for r in results) / len(results)\n",
        "avg_adapter_len = sum(r[\"adapter_len\"] for r in results) / len(results)\n",
        "\n",
        "timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "out_path = os.path.join(\n",
        "    \"/content/drive/MyDrive/멋사/comparison_dpo\", f\"adapter_comparison_{timestamp}.md\"\n",
        ") or args.out_path\n",
        "\n",
        "summary = {\n",
        "    \"csv\": args.csv_path,\n",
        "    \"adapter\": args.adapter_path,\n",
        "    \"samples\": len(results),\n",
        "    \"metrics\": [\n",
        "        f\"GPT wins: adapter {wins['adapter']} / base {wins['base']} (skip_llm_eval={args.skip_llm_eval})\",\n",
        "        f\"Avg coverage: adapter {avg_adapter_cov:.2f}, base {avg_base_cov:.2f}\",\n",
        "        f\"Avg tone match: adapter {avg_adapter_tone:.2f}, base {avg_base_tone:.2f}\",\n",
        "        f\"Avg style match: adapter {avg_adapter_style:.2f}, base {avg_base_style:.2f}\",\n",
        "        f\"Avg info density: adapter {avg_adapter_density:.2f}, base {avg_base_density:.2f}\",\n",
        "        f\"Repeat token ratio: adapter {avg_adapter_rep_token:.2f}, base {avg_base_rep_token:.2f}\",\n",
        "        f\"Repeat 3-gram ratio: adapter {avg_adapter_rep_ngram:.2f}, base {avg_base_rep_ngram:.2f}\",\n",
        "        f\"Length ok rate: adapter {adapter_len_ok_rate:.2f}, base {base_len_ok_rate:.2f}\",\n",
        "        f\"Forbidden violation rate: adapter {adapter_forbidden_rate:.2f}, base {base_forbidden_rate:.2f}\",\n",
        "        f\"CTA rate: adapter {adapter_cta_rate:.2f}, base {base_cta_rate:.2f}\",\n",
        "        f\"Avg length: adapter {avg_adapter_len:.1f}, base {avg_base_len:.1f}\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "_write_report(out_path, summary, results, args.max_examples)\n",
        "_log(f\"Saved report: {out_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_path = \"/content/drive/MyDrive/멋사/comparison_dpo/comparison_01.md\""
      ],
      "metadata": {
        "id": "8nJrC7YTyBfC",
        "outputId": "c48b20c3-9e66-4c13-e10c-334903d25175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "8nJrC7YTyBfC",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'content/drive/MyDrive/멋사/comparison_dpo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_write_report(out_path, summary, results, args.max_examples)\n",
        "_log(f\"Saved report: {out_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGx9-tcmZkII",
        "outputId": "537dffee-b314-4331-876e-c10a160633ef"
      },
      "id": "jGx9-tcmZkII",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['# Adapter Comparison Report', '', '- CSV: /content/AmoRe_crm_generator/finetuning/random_persona_campaign.csv', '- Adapter: /content/drive/MyDrive/멋사/adapters_dpo_2', '- Samples: 10', '', '## Summary', '', '- GPT wins: adapter 5 / base 5 (skip_llm_eval=False)', '- Avg coverage: adapter 0.33, base 0.24', '- Avg tone match: adapter 0.00, base 0.00', '- Avg style match: adapter 0.05, base 0.07', '- Avg info density: adapter 0.15, base 0.19', '- Repeat token ratio: adapter 0.06, base 0.04', '- Repeat 3-gram ratio: adapter 0.00, base 0.00', '- Length ok rate: adapter 0.00, base 0.10', '- Forbidden violation rate: adapter 0.00, base 0.00', '- CTA rate: adapter 1.00, base 0.80', '- Avg length: adapter 349.8, base 297.5', '', '## Per-sample Results', '', '| idx | persona | brand/product | stage | event | gpt winner | base len | adapter len | base cov | adapter cov | base tone | adapter tone | base style | adapter style | base dens | adapter dens | base rep tok | adapter rep tok | base rep 3g | adapter rep 3g | base len ok | adapter len ok | base forb | adapter forb | base cta | adapter cta |', '| --- | --- | --- | --- | --- | --- | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | --- | --- | ---: | ---: | --- | --- |', '', '## Examples', '']\n",
            "Saved report: /content/drive/MyDrive/멋사/comparison_dpo/comparison_01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "id": "35It-4GT3Cyl",
        "outputId": "4510f0af-badd-49da-e2a3-811a9f8f0bbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "35It-4GT3Cyl",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'csv': '/content/AmoRe_crm_generator/finetuning/random_persona_campaign.csv',\n",
              " 'adapter': '/content/drive/MyDrive/멋사/adapters_dpo_2',\n",
              " 'samples': 10,\n",
              " 'metrics': ['GPT wins: adapter 5 / base 5 (skip_llm_eval=False)',\n",
              "  'Avg coverage: adapter 0.33, base 0.24',\n",
              "  'Avg tone match: adapter 0.00, base 0.00',\n",
              "  'Avg style match: adapter 0.05, base 0.07',\n",
              "  'Avg info density: adapter 0.15, base 0.19',\n",
              "  'Repeat token ratio: adapter 0.06, base 0.04',\n",
              "  'Repeat 3-gram ratio: adapter 0.00, base 0.00',\n",
              "  'Length ok rate: adapter 0.00, base 0.10',\n",
              "  'Forbidden violation rate: adapter 0.00, base 0.00',\n",
              "  'CTA rate: adapter 1.00, base 0.80',\n",
              "  'Avg length: adapter 349.8, base 297.5']}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}