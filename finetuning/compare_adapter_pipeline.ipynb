{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c9f2ec",
   "metadata": {
    "id": "85c9f2ec"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "코랩용 기존 파이프라인 vs. 파인튜닝된 파이프라인 비교 스크립트\n",
    "./adapters_dpo에 존재하는 어댑터를 기준으로 GPT-Score / 비GPT-Score를 비교.\n",
    "random_persona_campaign.csv의 더미 데이터를 기준으로 평가함.\n",
    "비교 문서는 adapter_comparison_{timestamp}.md로 저장.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e06164",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8e06164",
    "outputId": "f5fb2a85-2c0e-4521-dec0-a90b8d4fbb7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e738c7ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e738c7ec",
    "outputId": "6365607b-b5a6-4a8f-c051-6890d8a17789"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
      "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.26.2)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.9.0+cu126)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.57.3)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.7.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
      "Name: transformers\n",
      "Version: 4.57.3\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /usr/local/lib/python3.12/dist-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: peft, sentence-transformers, trl\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets peft trl bitsandbytes accelerate\n",
    "!pip install -U transformers\n",
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42d8c26b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42d8c26b",
    "outputId": "2e5ba0d6-b520-4581-e027-6b42c8f8e2bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "716bd48b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "716bd48b",
    "outputId": "c2eac6a2-51a6-4418-eeb2-518f53d94704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "['.config', '.env', 'drive', '.ipynb_checkpoints', 'AmoRe_crm_generator', 'sample_data']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f903104c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f903104c",
    "outputId": "935c7226-ccd6-4fc4-c292-3925fcbb3fa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/AmoRe_crm_generator/finetuning\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/jjjh02/AmoRe_crm_generator.git\n",
    "%cd AmoRe_crm_generator\n",
    "!git checkout jinhyeok\n",
    "!git branch\n",
    "os.chdir(\"/content/AmoRe_crm_generator/finetuning\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "R7zWvPPsa673",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R7zWvPPsa673",
    "outputId": "7bfc2722-73e3-4c80-9ba9-8688ce602bcb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1e4a51",
   "metadata": {
    "id": "6f1e4a51"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import urllib.error\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "from contextlib import contextmanager\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "PROJECT_DIR = os.path.abspath(os.path.join(BASE_DIR, \"..\"))\n",
    "SRC_DIR = os.path.abspath(os.path.join(BASE_DIR, \"..\", \"src\"))\n",
    "DEFAULT_CSV = os.path.join(BASE_DIR, \"random_persona_campaign.csv\")\n",
    "DEFAULT_ADAPTER1_DIR = \"/content/drive/MyDrive/멋사/adapters_dpo_1_v2\"\n",
    "STAGE_ORDER = [\"Acquisition\", \"Activation\", \"Retention\", \"Revenue\", \"Referral\"]\n",
    "CANDIDATE_LABELS = [\"raw\", \"adapter1\"]\n",
    "\n",
    "print(BASE_DIR, SRC_DIR)\n",
    "\n",
    "def _log(message):\n",
    "    print(message)\n",
    "\n",
    "\n",
    "def _import_pipeline_module():\n",
    "    if SRC_DIR not in sys.path:\n",
    "        sys.path.insert(0, SRC_DIR)\n",
    "    try:\n",
    "        import run_qwen_exaone_pipeline as pipeline_module\n",
    "    except Exception as exc:\n",
    "        raise ImportError(\n",
    "            \"Failed to import main from ../src/run_qwen_exaone_pipeline.py\"\n",
    "        ) from exc\n",
    "    return pipeline_module\n",
    "\n",
    "\n",
    "def _load_json(path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _parse_bool(value):\n",
    "    if isinstance(value, bool):\n",
    "        return value\n",
    "    if value is None:\n",
    "        return False\n",
    "    if isinstance(value, (int, float)):\n",
    "        return bool(value)\n",
    "    text = str(value).strip().lower()\n",
    "    return text in {\"1\", \"true\", \"yes\", \"y\", \"t\"}\n",
    "\n",
    "\n",
    "def _load_rows(csv_path):\n",
    "    with open(csv_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            persona_raw = row.get(\"persona\", \"\").strip()\n",
    "            brand_raw = row.get(\"brand\", \"\").strip()\n",
    "            product_raw = row.get(\"product\", \"\").strip()\n",
    "            stage_raw = row.get(\"stage_index\", \"\").strip()\n",
    "            style_raw = row.get(\"style_index\", \"\").strip()\n",
    "            if not persona_raw or not brand_raw or not product_raw:\n",
    "                continue\n",
    "            if not stage_raw or not style_raw:\n",
    "                continue\n",
    "            try:\n",
    "                persona = int(persona_raw)\n",
    "                stage_index = int(stage_raw)\n",
    "                style_index = int(style_raw)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            yield {\n",
    "                \"persona\": persona,\n",
    "                \"brand\": brand_raw,\n",
    "                \"product\": product_raw,\n",
    "                \"stage_index\": stage_index,\n",
    "                \"style_index\": style_index,\n",
    "                \"is_event\": _parse_bool(row.get(\"is_event\", \"\")),\n",
    "            }\n",
    "\n",
    "\n",
    "def _get_stage_name(stage_index):\n",
    "    if isinstance(stage_index, int) and 0 <= stage_index < len(STAGE_ORDER):\n",
    "        return STAGE_ORDER[stage_index]\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def _get_crm_goal(crm_goals, stage_index, stage_name=None):\n",
    "    if not isinstance(crm_goals, dict):\n",
    "        return {}\n",
    "    if stage_name and stage_name in crm_goals:\n",
    "        return crm_goals.get(stage_name, {}) or {}\n",
    "    stage_name = _get_stage_name(stage_index)\n",
    "    if stage_name:\n",
    "        return crm_goals.get(stage_name, {}) or {}\n",
    "    return {}\n",
    "\n",
    "\n",
    "def _get_brand_story(brand_stories, brand_name):\n",
    "    if not isinstance(brand_stories, dict) or not brand_name:\n",
    "        return {}\n",
    "    if brand_name in brand_stories:\n",
    "        return brand_stories.get(brand_name, {}) or {}\n",
    "    for story in brand_stories.values():\n",
    "        if str(story.get(\"name_en\", \"\")).lower() == brand_name.lower():\n",
    "            return story\n",
    "    return {}\n",
    "\n",
    "\n",
    "def _format_event(selected_event):\n",
    "    if selected_event in (None, \"\", {}):\n",
    "        return \"없음\"\n",
    "    if isinstance(selected_event, dict):\n",
    "        for key in (\"title\", \"name\", \"event_name\", \"event\"):\n",
    "            if selected_event.get(key):\n",
    "                return str(selected_event.get(key))\n",
    "        return json.dumps(selected_event, ensure_ascii=False)\n",
    "    return str(selected_event)\n",
    "\n",
    "\n",
    "def _format_price(price):\n",
    "    if price in (None, \"\"):\n",
    "        return \"\"\n",
    "    if isinstance(price, (int, float)):\n",
    "        return f\"{int(price):,}원\"\n",
    "    text = str(price).strip()\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    if text.replace(\",\", \"\").isdigit():\n",
    "        return f\"{int(text.replace(',', '')):,}원\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def _format_persona(persona_profile):\n",
    "    if not isinstance(persona_profile, dict):\n",
    "        return str(persona_profile or \"\")\n",
    "    name = persona_profile.get(\"name\", \"\")\n",
    "    extras = []\n",
    "    value_focus = persona_profile.get(\"value_focus\")\n",
    "    skin_type = persona_profile.get(\"skin_type\")\n",
    "    traits = persona_profile.get(\"traits\")\n",
    "    shopping_style = persona_profile.get(\"shopping_style\")\n",
    "    if value_focus:\n",
    "        extras.append(str(value_focus))\n",
    "    if skin_type:\n",
    "        extras.append(str(skin_type))\n",
    "    if traits:\n",
    "        if isinstance(traits, list):\n",
    "            extras.append(\", \".join([str(t) for t in traits if t]))\n",
    "        else:\n",
    "            extras.append(str(traits))\n",
    "    if shopping_style:\n",
    "        extras.append(str(shopping_style))\n",
    "    extra_text = \", \".join([e for e in extras if e])\n",
    "    if name and extra_text:\n",
    "        return f\"{name} ({extra_text})\"\n",
    "    return name or extra_text\n",
    "\n",
    "\n",
    "def _build_context_block(out, max_style_templates=3):\n",
    "    persona = _format_persona(out.get(\"persona_profile\"))\n",
    "    stage = out.get(\"stage_name\") or out.get(\"stage_kr\") or \"\"\n",
    "    brand = out.get(\"brand\") or \"\"\n",
    "    product_basic = out.get(\"product_basic\") if isinstance(out.get(\"product_basic\"), dict) else {}\n",
    "    product_name = product_basic.get(\"name\") or out.get(\"product_query\") or \"\"\n",
    "    price = _format_price(product_basic.get(\"price\"))\n",
    "    objective = out.get(\"objective\") or \"\"\n",
    "    target_state = out.get(\"target_state\") or \"\"\n",
    "    style_templates = out.get(\"style_templates\") or []\n",
    "    if isinstance(style_templates, list):\n",
    "        style_templates = style_templates[:max_style_templates]\n",
    "    selected_event = _format_event(out.get(\"selected_event\"))\n",
    "\n",
    "    lines = [\"[컨텍스트]\"]\n",
    "    if persona:\n",
    "        lines.append(f\"- 페르소나: {persona}\")\n",
    "    if stage:\n",
    "        lines.append(f\"- 단계: {stage}\")\n",
    "    if brand or product_name:\n",
    "        if brand and product_name:\n",
    "            brand_product = f\"{brand} / {product_name}\"\n",
    "        else:\n",
    "            brand_product = brand or product_name\n",
    "        lines.append(f\"- 브랜드/제품: {brand_product}\")\n",
    "    if price:\n",
    "        lines.append(f\"- 가격: {price}\")\n",
    "    if objective:\n",
    "        lines.append(f\"- 목표: {objective}\")\n",
    "    if target_state:\n",
    "        lines.append(f\"- 목표 상태: {target_state}\")\n",
    "    if style_templates:\n",
    "        lines.append(\"- 스타일 템플릿:\")\n",
    "        for item in style_templates:\n",
    "            lines.append(f\"  - {item}\")\n",
    "    lines.append(f\"- 이벤트: {selected_event}\")\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "\n",
    "def _extract_message(out):\n",
    "    exaone = out.get(\"exaone\", {}) if isinstance(out, dict) else {}\n",
    "    return exaone.get(\"result_raw\") or \"\"\n",
    "\n",
    "\n",
    "def _tokenize(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    return [t for t in re.split(r\"\\s+\", str(text)) if len(t) > 1]\n",
    "\n",
    "\n",
    "def _split_tokens(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    cleaned = re.sub(r\"[^\\w\\uac00-\\ud7a3]+\", \" \", str(text), flags=re.UNICODE)\n",
    "    return [t for t in cleaned.split() if len(t) > 1]\n",
    "\n",
    "\n",
    "def _extract_keywords(texts, max_terms=30):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        for token in _split_tokens(text):\n",
    "            if token.isdigit():\n",
    "                continue\n",
    "            counter[token] += 1\n",
    "    if not counter:\n",
    "        return []\n",
    "    return [item for item, _ in counter.most_common(max_terms)]\n",
    "\n",
    "\n",
    "def _coverage_score(message, out):\n",
    "    total = 0\n",
    "    hits = 0\n",
    "    if not message:\n",
    "        return 0.0\n",
    "\n",
    "    brand = out.get(\"brand\")\n",
    "    if brand:\n",
    "        total += 1\n",
    "        if brand in message:\n",
    "            hits += 1\n",
    "\n",
    "    product_basic = out.get(\"product_basic\") if isinstance(out.get(\"product_basic\"), dict) else {}\n",
    "    product_name = product_basic.get(\"name\") or out.get(\"product_query\") or \"\"\n",
    "    if product_name:\n",
    "        total += 1\n",
    "        if product_name in message:\n",
    "            hits += 1\n",
    "\n",
    "    selected_event = _format_event(out.get(\"selected_event\"))\n",
    "    if selected_event and selected_event != \"없음\":\n",
    "        total += 1\n",
    "        if selected_event in message:\n",
    "            hits += 1\n",
    "\n",
    "    stage_terms = []\n",
    "    for text in (out.get(\"stage_kr\"), out.get(\"objective\"), out.get(\"target_state\")):\n",
    "        stage_terms.extend(_tokenize(text))\n",
    "    if stage_terms:\n",
    "        total += 1\n",
    "        if any(term in message for term in stage_terms):\n",
    "            hits += 1\n",
    "\n",
    "    return hits / total if total else 0.0\n",
    "\n",
    "\n",
    "def _tone_match_score(message, brand_story):\n",
    "    if not message or not isinstance(brand_story, dict):\n",
    "        return 0.0\n",
    "    tone_keywords = brand_story.get(\"tone_keywords\") or []\n",
    "    if not tone_keywords:\n",
    "        return 0.0\n",
    "    hits = sum(1 for kw in tone_keywords if kw and kw in message)\n",
    "    return hits / len(tone_keywords)\n",
    "\n",
    "\n",
    "def _style_match_score(message, style_templates, max_terms=30):\n",
    "    if not message or not style_templates:\n",
    "        return 0.0\n",
    "    if not isinstance(style_templates, list):\n",
    "        style_templates = [str(style_templates)]\n",
    "    keywords = _extract_keywords(style_templates, max_terms=max_terms)\n",
    "    if not keywords:\n",
    "        return 0.0\n",
    "    hits = sum(1 for kw in keywords if kw in message)\n",
    "    return hits / len(keywords)\n",
    "\n",
    "\n",
    "def _info_density(message, out):\n",
    "    if not message:\n",
    "        return 0.0\n",
    "    persona = out.get(\"persona_profile\") if isinstance(out.get(\"persona_profile\"), dict) else {}\n",
    "    product_basic = out.get(\"product_basic\") if isinstance(out.get(\"product_basic\"), dict) else {}\n",
    "    context_texts = [\n",
    "        out.get(\"brand\"),\n",
    "        product_basic.get(\"name\"),\n",
    "        out.get(\"product_query\"),\n",
    "        out.get(\"stage_kr\"),\n",
    "        out.get(\"objective\"),\n",
    "        out.get(\"target_state\"),\n",
    "        persona.get(\"value_focus\"),\n",
    "        persona.get(\"skin_type\"),\n",
    "    ]\n",
    "    if isinstance(persona.get(\"traits\"), list):\n",
    "        context_texts.extend(persona.get(\"traits\"))\n",
    "    if persona.get(\"shopping_style\"):\n",
    "        context_texts.append(persona.get(\"shopping_style\"))\n",
    "\n",
    "    keywords = _extract_keywords([t for t in context_texts if t], max_terms=40)\n",
    "    if not keywords:\n",
    "        return 0.0\n",
    "    message_tokens = _split_tokens(message)\n",
    "    if not message_tokens:\n",
    "        return 0.0\n",
    "    hits = sum(1 for kw in keywords if kw in message)\n",
    "    return hits / len(message_tokens)\n",
    "\n",
    "\n",
    "def _repetition_stats(message):\n",
    "    tokens = _split_tokens(message)\n",
    "    if not tokens:\n",
    "        return 0.0, 0.0\n",
    "    unique_tokens = set(tokens)\n",
    "    repeat_token_ratio = (len(tokens) - len(unique_tokens)) / len(tokens)\n",
    "\n",
    "    if len(tokens) < 6:\n",
    "        return repeat_token_ratio, 0.0\n",
    "    n = 3\n",
    "    ngrams = [\" \".join(tokens[i:i + n]) for i in range(len(tokens) - n + 1)]\n",
    "    counts = Counter(ngrams)\n",
    "    total_ngrams = len(ngrams)\n",
    "    repeated = sum(count - 1 for count in counts.values() if count > 1)\n",
    "    repeat_ngram_ratio = repeated / total_ngrams if total_ngrams else 0.0\n",
    "    return repeat_token_ratio, repeat_ngram_ratio\n",
    "\n",
    "\n",
    "def _length_target(stage_name):\n",
    "    if stage_name == \"Acquisition\":\n",
    "        return 60, 200\n",
    "    if stage_name == \"Activation\":\n",
    "        return 60, 200\n",
    "    if stage_name == \"Retention\":\n",
    "        return 60, 180\n",
    "    if stage_name == \"Revenue\":\n",
    "        return 60, 180\n",
    "    if stage_name == \"Referral\":\n",
    "        return 60, 160\n",
    "    return 50, 220\n",
    "\n",
    "\n",
    "def _length_ok(message, stage_name):\n",
    "    if not message:\n",
    "        return False\n",
    "    min_len, max_len = _length_target(stage_name)\n",
    "    return min_len <= len(message) <= max_len\n",
    "\n",
    "\n",
    "def _forbidden_violations(message, crm_goal):\n",
    "    if not message or not isinstance(crm_goal, dict):\n",
    "        return 0\n",
    "    forbidden = crm_goal.get(\"forbidden_context\") or []\n",
    "    if not forbidden:\n",
    "        return 0\n",
    "    hits = 0\n",
    "    for term in forbidden:\n",
    "        if term and term in message:\n",
    "            hits += 1\n",
    "    return hits\n",
    "\n",
    "\n",
    "def _cta_present(message):\n",
    "    if not message:\n",
    "        return False\n",
    "    cta_markers = [\n",
    "        \"지금\", \"확인\", \"구매\", \"신청\", \"참여\",\n",
    "        \"클릭\", \"받기\", \"혜택\", \"할인\", \"쿠폰\",\n",
    "        \"해보세요\", \"하세요\", \"둘러보기\",\n",
    "        \"바로\", \"추천\", \"문의\"\n",
    "    ]\n",
    "    return any(marker in message for marker in cta_markers)\n",
    "\n",
    "\n",
    "def _score_message(message, base_out, brand_story, crm_goal, stage_name):\n",
    "    coverage = _coverage_score(message, base_out)\n",
    "    tone = _tone_match_score(message, brand_story)\n",
    "    style = _style_match_score(message, base_out.get(\"style_templates\"))\n",
    "    density = _info_density(message, base_out)\n",
    "    rep_token, rep_ngram = _repetition_stats(message)\n",
    "    length_ok = _length_ok(message, stage_name)\n",
    "    forbidden = _forbidden_violations(message, crm_goal)\n",
    "    length = len(message)\n",
    "    cta = _cta_present(message)\n",
    "    return {\n",
    "        \"len\": length,\n",
    "        \"cov\": coverage,\n",
    "        \"tone\": tone,\n",
    "        \"style\": style,\n",
    "        \"density\": density,\n",
    "        \"rep_token\": rep_token,\n",
    "        \"rep_ngram\": rep_ngram,\n",
    "        \"len_ok\": length_ok,\n",
    "        \"forbidden\": forbidden,\n",
    "        \"cta\": cta,\n",
    "    }\n",
    "\n",
    "\n",
    "def _call_gpt(context_block, messages):\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY is not set.\")\n",
    "\n",
    "    candidate_block = \"\\n\\n\".join(\n",
    "        f\"[{idx}]\\n{msg if msg else '(빈 메시지)'}\" for idx, msg in enumerate(messages)\n",
    "    )\n",
    "\n",
    "    system_prompt = (\n",
    "        \"너는 마케팅 문장 평가자다.\\n\"\n",
    "        \"목표는 전환 가능성이 더 높은 CRM 메시지를 고르는 것이다.\\n\\n\"\n",
    "        \"다음 기준으로 후보를 비교하라:\\n\"\n",
    "        \"1. 수신자가 실제 행동(클릭/재구매)을 할 가능성\\n\"\n",
    "        \"2. persona와 구매 단계 적합성\\n\"\n",
    "        \"3. 상품·브랜드 핵심 장점 전달력\\n\"\n",
    "        \"4. 스타일 템플릿/이벤트 정보를 적절히 반영했는가\\n\"\n",
    "        \"5. 불필요한 장식 없이 명확한가\\n\\n\"\n",
    "        \"가장 좋은 후보의 번호만 0 또는 1로 출력하라.\"\n",
    "    )\n",
    "    user_prompt = (\n",
    "        \"컨텍스트:\\n\"\n",
    "        f\"{context_block}\\n\\n\"\n",
    "        \"후보:\\n\"\n",
    "        f\"{candidate_block}\\n\\n\"\n",
    "        \"번호만 답해라.\"\n",
    "    )\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-5-nano\",\n",
    "        \"input\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"input_text\", \"text\": system_prompt}],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"input_text\", \"text\": user_prompt}],\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    request = urllib.request.Request(\n",
    "        \"https://api.openai.com/v1/responses\",\n",
    "        data=json.dumps(payload).encode(\"utf-8\"),\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        },\n",
    "        method=\"POST\",\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        with urllib.request.urlopen(request, timeout=30) as response:\n",
    "            data = json.loads(response.read().decode(\"utf-8\"))\n",
    "    except urllib.error.HTTPError as exc:\n",
    "        body = exc.read().decode(\"utf-8\", errors=\"replace\")\n",
    "        raise RuntimeError(f\"OpenAI API error {exc.code}: {body}\") from exc\n",
    "\n",
    "    output_text = _extract_response_text(data)\n",
    "    match = re.search(r\"-?\\d+\", str(output_text))\n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid evaluator response: {output_text}\")\n",
    "    choice = int(match.group(0))\n",
    "    if choice not in (0, 1):\n",
    "        raise ValueError(f\"Evaluator index out of range: {choice}\")\n",
    "    return choice\n",
    "\n",
    "\n",
    "def _extract_response_text(data):\n",
    "    if isinstance(data, dict):\n",
    "        output_text = data.get(\"output_text\")\n",
    "        if isinstance(output_text, str) and output_text.strip():\n",
    "            return output_text.strip()\n",
    "\n",
    "        output = data.get(\"output\")\n",
    "        if isinstance(output, list):\n",
    "            parts = []\n",
    "            for item in output:\n",
    "                if not isinstance(item, dict):\n",
    "                    continue\n",
    "                content = item.get(\"content\", [])\n",
    "                if isinstance(content, list):\n",
    "                    for block in content:\n",
    "                        if isinstance(block, dict) and isinstance(block.get(\"text\"), str):\n",
    "                            parts.append(block[\"text\"])\n",
    "                        elif isinstance(block, str):\n",
    "                            parts.append(block)\n",
    "                elif isinstance(content, str):\n",
    "                    parts.append(content)\n",
    "            if parts:\n",
    "                return \"\".join(parts).strip()\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def _patch_exaone(pipeline_module, adapter_path=None, adapter_paths=None):\n",
    "    import tone_correction\n",
    "\n",
    "    class PatchedExaoneToneCorrector(tone_correction.ExaoneToneCorrector):\n",
    "        _cache = {}\n",
    "\n",
    "        def __init__(self, model_name=\"LGAI-EXAONE/EXAONE-4.0-1.2B\"):\n",
    "            if adapter_paths:\n",
    "                key = (model_name, tuple(adapter_paths))\n",
    "            else:\n",
    "                key = (model_name, adapter_path)\n",
    "            cached = self._cache.get(key)\n",
    "            if cached:\n",
    "                self.device = cached[\"device\"]\n",
    "                self.model_name = model_name\n",
    "                self.tokenizer = cached[\"tokenizer\"]\n",
    "                self.model = cached[\"model\"]\n",
    "                return\n",
    "            super().__init__(model_name=model_name)\n",
    "            if adapter_paths:\n",
    "                self._apply_adapters(adapter_paths)\n",
    "            elif adapter_path:\n",
    "                self._apply_adapters([adapter_path])\n",
    "            self._cache[key] = {\n",
    "                \"device\": self.device,\n",
    "                \"tokenizer\": self.tokenizer,\n",
    "                \"model\": self.model,\n",
    "            }\n",
    "\n",
    "        def _apply_adapters(self, paths):\n",
    "            if not paths:\n",
    "                return\n",
    "            try:\n",
    "                from peft import PeftModel\n",
    "            except ImportError as exc:\n",
    "                raise RuntimeError(\"peft is required to load adapters.\") from exc\n",
    "\n",
    "            self.model = PeftModel.from_pretrained(self.model, paths[0])\n",
    "            if len(paths) == 1:\n",
    "                try:\n",
    "                    self.model.eval()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                return\n",
    "\n",
    "            merged = None\n",
    "            try:\n",
    "                merged = self.model.merge_and_unload()\n",
    "            except Exception:\n",
    "                merged = None\n",
    "\n",
    "            if merged is not None:\n",
    "                self.model = merged\n",
    "                self.model = PeftModel.from_pretrained(self.model, paths[1])\n",
    "            else:\n",
    "                try:\n",
    "                    self.model.load_adapter(paths[1], adapter_name=\"adapter2\")\n",
    "                    try:\n",
    "                        self.model.set_adapter([\"default\", \"adapter2\"])\n",
    "                    except Exception:\n",
    "                        self.model.set_adapter(\"adapter2\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            try:\n",
    "                self.model.eval()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    original = pipeline_module.ExaoneToneCorrector\n",
    "    pipeline_module.ExaoneToneCorrector = PatchedExaoneToneCorrector\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        pipeline_module.ExaoneToneCorrector = original\n",
    "\n",
    "\n",
    "def _run_pipeline_main(pipeline_main, row):\n",
    "    argv = [\n",
    "        \"run_qwen_exaone_pipeline.py\",\n",
    "        \"--persona\",\n",
    "        str(row[\"persona\"]),\n",
    "        \"--brand\",\n",
    "        row[\"brand\"],\n",
    "        \"--product\",\n",
    "        row[\"product\"],\n",
    "        \"--stage_index\",\n",
    "        str(row[\"stage_index\"]),\n",
    "        \"--style_index\",\n",
    "        str(row[\"style_index\"]),\n",
    "        \"--is_event\",\n",
    "        \"1\" if row.get(\"is_event\", False) else \"0\",\n",
    "    ]\n",
    "    old_argv = sys.argv\n",
    "    try:\n",
    "        sys.argv = argv\n",
    "        return pipeline_main()\n",
    "    finally:\n",
    "        sys.argv = old_argv\n",
    "\n",
    "\n",
    "def _row_key(row):\n",
    "    return \"{persona}|{brand}|{product}|{stage}|{style}|{event}\".format(\n",
    "        persona=row.get(\"persona\", \"\"),\n",
    "        brand=row.get(\"brand\", \"\"),\n",
    "        product=row.get(\"product\", \"\"),\n",
    "        stage=row.get(\"stage_index\", \"\"),\n",
    "        style=row.get(\"style_index\", \"\"),\n",
    "        event=int(bool(row.get(\"is_event\", False))),\n",
    "    )\n",
    "\n",
    "\n",
    "def _load_checkpoint(checkpoint_path, row_key_map):\n",
    "    if not checkpoint_path or not os.path.exists(checkpoint_path):\n",
    "        return [], set()\n",
    "    results = []\n",
    "    with open(checkpoint_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                item = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            row_key = item.get(\"row_key\")\n",
    "            if row_key and row_key in row_key_map:\n",
    "                item[\"idx\"] = row_key_map[row_key]\n",
    "            if \"idx\" not in item:\n",
    "                continue\n",
    "            results.append(item)\n",
    "    by_idx = {}\n",
    "    for item in results:\n",
    "        by_idx[item[\"idx\"]] = item\n",
    "    ordered = [by_idx[idx] for idx in sorted(by_idx)]\n",
    "    return ordered, set(by_idx)\n",
    "\n",
    "\n",
    "def _append_checkpoint(checkpoint_path, item):\n",
    "    if not checkpoint_path:\n",
    "        return\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    if checkpoint_dir:\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    with open(checkpoint_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "def _count_wins(results):\n",
    "    wins = {\"raw\": 0, \"adapter1\": 0}\n",
    "    mapping = {\n",
    "        \"raw\": \"raw\",\n",
    "        \"ad1\": \"adapter1\",\n",
    "        \"adapter1\": \"adapter1\",\n",
    "    }\n",
    "    for item in results:\n",
    "        key = mapping.get(item.get(\"winner\"))\n",
    "        if key:\n",
    "            wins[key] += 1\n",
    "    return wins\n",
    "\n",
    "\n",
    "def _write_report(out_path, summary, rows, max_examples):\n",
    "    lines = []\n",
    "    lines.append(\"# 어댑터 비교 리포트\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"- CSV: {summary['csv']}\")\n",
    "    lines.append(f\"- 어댑터1: {summary['adapter1']}\")\n",
    "    lines.append(f\"- 샘플 수: {summary['samples']}\")\n",
    "    lines.append(\"- 표기: raw=기본 모델, ad1=어댑터1\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## 요약\")\n",
    "    lines.append(\"\")\n",
    "    for item in summary[\"metrics\"]:\n",
    "        lines.append(f\"- {item}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## 지표 설명\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"- GPT 승자: gpt-5-nano가 동일 컨텍스트 기준으로 2개 후보 중 더 좋은 메시지를 선택한 결과.\")\n",
    "    lines.append(\"- 커버리지: 브랜드/제품/이벤트/스테이지 관련 용어가 메시지에 포함된 비율.\")\n",
    "    lines.append(\"- 톤 일치율: 브랜드 톤 키워드가 메시지에 포함된 비율.\")\n",
    "    lines.append(\"- 스타일 일치율: 스타일 템플릿에서 뽑은 키워드 포함 비율.\")\n",
    "    lines.append(\"- 정보 밀도: 컨텍스트 키워드 적중 수 / 메시지 토큰 수.\")\n",
    "    lines.append(\"- 반복 토큰 비율: (토큰 수 - 고유 토큰 수) / 토큰 수.\")\n",
    "    lines.append(\"- 반복 3-그램 비율: 반복된 3-그램 수 / 전체 3-그램 수.\")\n",
    "    lines.append(\"- 길이 적정: 스테이지별 권장 길이 범위 충족 여부.\")\n",
    "    lines.append(\"- 금지 맥락 위반율: forbidden_context 용어가 포함된 메시지 비율.\")\n",
    "    lines.append(\"- CTA 비율: CTA 키워드가 포함된 메시지 비율.\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## 샘플별 결과\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\n",
    "        \"| idx | persona | 브랜드/제품 | 스테이지 | 이벤트 | GPT 승자 | raw 길이 | ad1 길이 | raw 커버리지 | ad1 커버리지 | raw 톤 | ad1 톤 | raw 스타일 | ad1 스타일 | raw 밀도 | ad1 밀도 | raw 반복 토큰 | ad1 반복 토큰 | raw 반복 3g | ad1 반복 3g | raw 길이 적정 | ad1 길이 적정 | raw 금지 | ad1 금지 | raw CTA | ad1 CTA |\"\n",
    "    )\n",
    "    lines.append(\n",
    "        \"| --- | --- | --- | --- | --- | --- | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | --- | --- | ---: | ---: | --- | --- |\"\n",
    "    )\n",
    "    for item in rows:\n",
    "        lines.append(\n",
    "            \"| {idx} | {persona} | {brand_product} | {stage} | {event} | {winner} | {raw_len} | {ad1_len} | {raw_cov:.2f} | {ad1_cov:.2f} | {raw_tone:.2f} | {ad1_tone:.2f} | {raw_style:.2f} | {ad1_style:.2f} | {raw_density:.2f} | {ad1_density:.2f} | {raw_rep_token:.2f} | {ad1_rep_token:.2f} | {raw_rep_ngram:.2f} | {ad1_rep_ngram:.2f} | {raw_len_ok} | {ad1_len_ok} | {raw_forbidden} | {ad1_forbidden} | {raw_cta} | {ad1_cta} |\".format(\n",
    "                idx=item[\"idx\"],\n",
    "                persona=item[\"persona\"],\n",
    "                brand_product=item[\"brand_product\"],\n",
    "                stage=item[\"stage\"],\n",
    "                event=item[\"event\"],\n",
    "                winner=item[\"winner\"],\n",
    "                raw_len=item[\"raw_len\"],\n",
    "                ad1_len=item[\"adapter1_len\"],\n",
    "                raw_cov=item[\"raw_cov\"],\n",
    "                ad1_cov=item[\"adapter1_cov\"],\n",
    "                raw_tone=item[\"raw_tone\"],\n",
    "                ad1_tone=item[\"adapter1_tone\"],\n",
    "                raw_style=item[\"raw_style\"],\n",
    "                ad1_style=item[\"adapter1_style\"],\n",
    "                raw_density=item[\"raw_density\"],\n",
    "                ad1_density=item[\"adapter1_density\"],\n",
    "                raw_rep_token=item[\"raw_rep_token\"],\n",
    "                ad1_rep_token=item[\"adapter1_rep_token\"],\n",
    "                raw_rep_ngram=item[\"raw_rep_ngram\"],\n",
    "                ad1_rep_ngram=item[\"adapter1_rep_ngram\"],\n",
    "                raw_len_ok=\"yes\" if item[\"raw_len_ok\"] else \"no\",\n",
    "                ad1_len_ok=\"yes\" if item[\"adapter1_len_ok\"] else \"no\",\n",
    "                raw_forbidden=item[\"raw_forbidden\"],\n",
    "                ad1_forbidden=item[\"adapter1_forbidden\"],\n",
    "                raw_cta=\"yes\" if item[\"raw_cta\"] else \"no\",\n",
    "                ad1_cta=\"yes\" if item[\"adapter1_cta\"] else \"no\",\n",
    "            )\n",
    "        )\n",
    "    lines.append(\"\")\n",
    "\n",
    "    example_count = min(max_examples, len(rows))\n",
    "    if example_count > 0:\n",
    "        lines.append(\"## 예시\")\n",
    "        lines.append(\"\")\n",
    "        for item in rows[:example_count]:\n",
    "            lines.append(f\"### 샘플 {item['idx']}\")\n",
    "            lines.append(\"\")\n",
    "            lines.append(\"컨텍스트:\")\n",
    "            lines.append(\"\")\n",
    "            lines.append(\"```\")\n",
    "            lines.append(item[\"context\"])\n",
    "            lines.append(\"```\")\n",
    "            lines.append(\"\")\n",
    "            lines.append(\"Raw 메시지:\")\n",
    "            lines.append(\"\")\n",
    "            lines.append(\"```\")\n",
    "            lines.append(item[\"raw_message\"] or \"(빈 메시지)\")\n",
    "            lines.append(\"```\")\n",
    "            lines.append(\"\")\n",
    "            lines.append(\"Adapter1 메시지:\")\n",
    "            lines.append(\"\")\n",
    "            lines.append(\"```\")\n",
    "            lines.append(item[\"adapter1_message\"] or \"(빈 메시지)\")\n",
    "            lines.append(\"```\")\n",
    "            lines.append(\"\")\n",
    "            lines.append(f\"GPT 승자: {item['winner']}\")\n",
    "            lines.append(\"\")\n",
    "    else:\n",
    "        lines.append(\"## 예시\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"예시가 없습니다 (max_examples가 0이거나 처리된 행이 없습니다).\")\n",
    "        lines.append(\"\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b122a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--csv_path\", default=DEFAULT_CSV)\n",
    "parser.add_argument(\"--adapter1_path\", default=DEFAULT_ADAPTER1_DIR)\n",
    "parser.add_argument(\"--out_path\", default=None)\n",
    "parser.add_argument(\"--checkpoint_path\", default=None)\n",
    "parser.add_argument(\"--max_rows\", type=int, default=None)\n",
    "parser.add_argument(\"--max_examples\", type=int, default=3)\n",
    "parser.add_argument(\"--skip_llm_eval\", action=\"store_true\")\n",
    "parser.add_argument(\"--max_style_templates\", type=int, default=3)\n",
    "args = parser.parse_args()\n",
    "\n",
    "if not os.path.exists(args.csv_path):\n",
    "    raise FileNotFoundError(f\"CSV not found: {args.csv_path}\")\n",
    "if not os.path.exists(args.adapter1_path):\n",
    "    raise FileNotFoundError(f\"Adapter not found: {args.adapter1_path}\")\n",
    "\n",
    "pipeline_module = _import_pipeline_module()\n",
    "pipeline_main = pipeline_module.main\n",
    "brand_stories = _load_json(os.path.join(PROJECT_DIR, \"data\", \"brand_stories.json\"))\n",
    "crm_goals = _load_json(os.path.join(PROJECT_DIR, \"data\", \"crm_goals.json\"))\n",
    "\n",
    "rows = []\n",
    "row_key_map = {}\n",
    "for idx, row in enumerate(_load_rows(args.csv_path), start=1):\n",
    "    if args.max_rows is not None and idx > args.max_rows:\n",
    "        break\n",
    "    rows.append(row)\n",
    "    key = _row_key(row)\n",
    "    if key in row_key_map:\n",
    "        _log(f\"[WARN] Duplicate row key at idx={idx}.\")\n",
    "    else:\n",
    "        row_key_map[key] = idx\n",
    "\n",
    "if not rows:\n",
    "    raise RuntimeError(\"No rows to evaluate.\")\n",
    "\n",
    "checkpoint_path = args.checkpoint_path or os.path.join(\n",
    "    BASE_DIR, \"adapter_comparison_2way_checkpoint.jsonl\"\n",
    ")\n",
    "results, processed = _load_checkpoint(checkpoint_path, row_key_map)\n",
    "if results:\n",
    "    _log(f\"Resume from checkpoint: {checkpoint_path} ({len(results)} rows)\")\n",
    "\n",
    "wins = _count_wins(results)\n",
    "\n",
    "for idx, row in enumerate(rows, start=1):\n",
    "    if idx in processed:\n",
    "        _log(f\"[Row {idx}] skipped (checkpoint)\")\n",
    "        continue\n",
    "    _log(\n",
    "        \"[Row {idx}] persona={persona} brand={brand} product={product} \"\n",
    "        \"stage_index={stage_index} style_index={style_index} is_event={is_event}\".format(\n",
    "            idx=idx,\n",
    "            persona=row[\"persona\"],\n",
    "            brand=row[\"brand\"],\n",
    "            product=row[\"product\"],\n",
    "            stage_index=row[\"stage_index\"],\n",
    "            style_index=row[\"style_index\"],\n",
    "            is_event=row.get(\"is_event\", False),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    _log(\"  Running raw pipeline...\")\n",
    "    raw_out = _run_pipeline_main(pipeline_main, row)\n",
    "\n",
    "    _log(\"  Running adapter1 pipeline...\")\n",
    "    with _patch_exaone(pipeline_module, adapter_path=args.adapter1_path):\n",
    "        adapter1_out = _run_pipeline_main(pipeline_main, row)\n",
    "\n",
    "    raw_message = _extract_message(raw_out)\n",
    "    adapter1_message = _extract_message(adapter1_out)\n",
    "\n",
    "    context_block = _build_context_block(raw_out, args.max_style_templates)\n",
    "    stage_name = raw_out.get(\"stage_name\") or _get_stage_name(row[\"stage_index\"])\n",
    "    crm_goal = _get_crm_goal(crm_goals, row[\"stage_index\"], stage_name)\n",
    "    brand_story = _get_brand_story(brand_stories, raw_out.get(\"brand\"))\n",
    "\n",
    "    winner = \"n/a\"\n",
    "    if not args.skip_llm_eval:\n",
    "        choice = _call_gpt(context_block, [raw_message, adapter1_message])\n",
    "        winner = CANDIDATE_LABELS[choice]\n",
    "        wins[winner] += 1\n",
    "\n",
    "    raw_metrics = _score_message(raw_message, raw_out, brand_story, crm_goal, stage_name)\n",
    "    adapter1_metrics = _score_message(adapter1_message, raw_out, brand_story, crm_goal, stage_name)\n",
    "\n",
    "    winner_short = {\"raw\": \"raw\", \"adapter1\": \"ad1\"}.get(winner, \"n/a\")\n",
    "    row_key = _row_key(row)\n",
    "\n",
    "    result = {\n",
    "        \"idx\": idx,\n",
    "        \"row_key\": row_key,\n",
    "        \"persona\": row[\"persona\"],\n",
    "        \"brand_product\": f\"{row['brand']} / {row['product']}\",\n",
    "        \"stage\": raw_out.get(\"stage_name\") or raw_out.get(\"stage_kr\") or \"\",\n",
    "        \"event\": _format_event(raw_out.get(\"selected_event\")),\n",
    "        \"winner\": winner_short,\n",
    "        \"raw_len\": raw_metrics[\"len\"],\n",
    "        \"adapter1_len\": adapter1_metrics[\"len\"],\n",
    "        \"raw_cov\": raw_metrics[\"cov\"],\n",
    "        \"adapter1_cov\": adapter1_metrics[\"cov\"],\n",
    "        \"raw_tone\": raw_metrics[\"tone\"],\n",
    "        \"adapter1_tone\": adapter1_metrics[\"tone\"],\n",
    "        \"raw_style\": raw_metrics[\"style\"],\n",
    "        \"adapter1_style\": adapter1_metrics[\"style\"],\n",
    "        \"raw_density\": raw_metrics[\"density\"],\n",
    "        \"adapter1_density\": adapter1_metrics[\"density\"],\n",
    "        \"raw_rep_token\": raw_metrics[\"rep_token\"],\n",
    "        \"adapter1_rep_token\": adapter1_metrics[\"rep_token\"],\n",
    "        \"raw_rep_ngram\": raw_metrics[\"rep_ngram\"],\n",
    "        \"adapter1_rep_ngram\": adapter1_metrics[\"rep_ngram\"],\n",
    "        \"raw_len_ok\": raw_metrics[\"len_ok\"],\n",
    "        \"adapter1_len_ok\": adapter1_metrics[\"len_ok\"],\n",
    "        \"raw_forbidden\": raw_metrics[\"forbidden\"],\n",
    "        \"adapter1_forbidden\": adapter1_metrics[\"forbidden\"],\n",
    "        \"raw_cta\": raw_metrics[\"cta\"],\n",
    "        \"adapter1_cta\": adapter1_metrics[\"cta\"],\n",
    "        \"context\": context_block,\n",
    "        \"raw_message\": raw_message,\n",
    "        \"adapter1_message\": adapter1_message,\n",
    "    }\n",
    "    results.append(result)\n",
    "    _append_checkpoint(checkpoint_path, result)\n",
    "\n",
    "\n",
    "def _avg_metric(results, key):\n",
    "    values = [r[key] for r in results if key in r]\n",
    "    return sum(values) / len(values) if values else 0.0\n",
    "\n",
    "\n",
    "results = sorted(results, key=lambda item: item.get(\"idx\", 0))\n",
    "total = len(results) if results else 1\n",
    "\n",
    "avg_cov = {c: _avg_metric(results, f\"{c}_cov\") for c in (\"raw\", \"adapter1\")}\n",
    "avg_tone = {c: _avg_metric(results, f\"{c}_tone\") for c in (\"raw\", \"adapter1\")}\n",
    "avg_style = {c: _avg_metric(results, f\"{c}_style\") for c in (\"raw\", \"adapter1\")}\n",
    "avg_density = {c: _avg_metric(results, f\"{c}_density\") for c in (\"raw\", \"adapter1\")}\n",
    "avg_rep_token = {c: _avg_metric(results, f\"{c}_rep_token\") for c in (\"raw\", \"adapter1\")}\n",
    "avg_rep_ngram = {c: _avg_metric(results, f\"{c}_rep_ngram\") for c in (\"raw\", \"adapter1\")}\n",
    "len_ok_rate = {\n",
    "    c: sum(1 for r in results if r.get(f\"{c}_len_ok\")) / total\n",
    "    for c in (\"raw\", \"adapter1\")\n",
    "}\n",
    "forbidden_rate = {\n",
    "    c: sum(1 for r in results if r.get(f\"{c}_forbidden\", 0) > 0) / total\n",
    "    for c in (\"raw\", \"adapter1\")\n",
    "}\n",
    "cta_rate = {\n",
    "    c: sum(1 for r in results if r.get(f\"{c}_cta\")) / total\n",
    "    for c in (\"raw\", \"adapter1\")\n",
    "}\n",
    "avg_len = {c: _avg_metric(results, f\"{c}_len\") for c in (\"raw\", \"adapter1\")}\n",
    "\n",
    "timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "out_path = args.out_path or os.path.join(\n",
    "    BASE_DIR, f\"adapter_comparison_2way_{timestamp}.md\"\n",
    ")\n",
    "\n",
    "summary = {\n",
    "    \"csv\": args.csv_path,\n",
    "    \"adapter1\": args.adapter1_path,\n",
    "    \"samples\": len(results),\n",
    "    \"metrics\": [\n",
    "        \"GPT 승자: raw {raw} / ad1 {ad1} (skip_llm_eval={skip})\".format(\n",
    "            raw=wins[\"raw\"],\n",
    "            ad1=wins[\"adapter1\"],\n",
    "            skip=args.skip_llm_eval,\n",
    "        ),\n",
    "        \"평균 커버리지: raw {raw:.2f}, ad1 {ad1:.2f}\".format(\n",
    "            raw=avg_cov[\"raw\"], ad1=avg_cov[\"adapter1\"]\n",
    "        ),\n",
    "        \"평균 톤 일치율: raw {raw:.2f}, ad1 {ad1:.2f}\".format(\n",
    "            raw=avg_tone[\"raw\"], ad1=avg_tone[\"adapter1\"]\n",
    "        ),\n",
    "        \"평균 스타일 일치율: raw {raw:.2f}, ad1 {ad1:.2f}\".format(\n",
    "            raw=avg_style[\"raw\"], ad1=avg_style[\"adapter1\"]\n",
    "        ),\n",
    "        \"평균 정보 밀도: raw {raw:.2f}, ad1 {ad1:.2f}\".format(\n",
    "            raw=avg_density[\"raw\"], ad1=avg_density[\"adapter1\"]\n",
    "        ),\n",
    "        \"반복 토큰 비율: raw {raw:.2f}, ad1 {ad1:.2f}\".format(\n",
    "            raw=avg_rep_token[\"raw\"], ad1=avg_rep_token[\"adapter1\"]\n",
    "        ),\n",
    "        \"반복 3-그램 비율: raw {raw:.2f}, ad1 {ad1:.2f}\".format(\n",
    "            raw=avg_rep_ngram[\"raw\"], ad1=avg_rep_ngram[\"adapter1\"]\n",
    "        ),\n",
    "        \"길이 적정 비율: raw {raw:.2f}, ad1 {ad1:.2f}\".format(\n",
    "            raw=len_ok_rate[\"raw\"], ad1=len_ok_rate[\"adapter1\"]\n",
    "        ),\n",
    "        \"금지 맥락 위반 비율: raw {raw:.2f}, ad1 {ad1:.2f}\".format(\n",
    "            raw=forbidden_rate[\"raw\"], ad1=forbidden_rate[\"adapter1\"]\n",
    "        ),\n",
    "        \"CTA 비율: raw {raw:.2f}, ad1 {ad1:.2f}\".format(\n",
    "            raw=cta_rate[\"raw\"], ad1=cta_rate[\"adapter1\"]\n",
    "        ),\n",
    "        \"평균 길이: raw {raw:.1f}, ad1 {ad1:.1f}\".format(\n",
    "            raw=avg_len[\"raw\"], ad1=avg_len[\"adapter1\"]\n",
    "        ),\n",
    "    ],\n",
    "}\n",
    "\n",
    "_write_report(out_path, summary, results, args.max_examples)\n",
    "_log(f\"Saved report: {out_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8nJrC7YTyBfC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "8nJrC7YTyBfC",
    "outputId": "c48b20c3-9e66-4c13-e10c-334903d25175"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'content/drive/MyDrive/멋사/comparison_dpo'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_path = \"/content/drive/MyDrive/\\uba4\\uc0ac/comparison_dpo/comparison_3way_stack_01.md\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "jGx9-tcmZkII",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jGx9-tcmZkII",
    "outputId": "537dffee-b314-4331-876e-c10a160633ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# Adapter Comparison Report', '', '- CSV: /content/AmoRe_crm_generator/finetuning/random_persona_campaign.csv', '- Adapter: /content/drive/MyDrive/멋사/adapters_dpo_2', '- Samples: 10', '', '## Summary', '', '- GPT wins: adapter 5 / base 5 (skip_llm_eval=False)', '- Avg coverage: adapter 0.33, base 0.24', '- Avg tone match: adapter 0.00, base 0.00', '- Avg style match: adapter 0.05, base 0.07', '- Avg info density: adapter 0.15, base 0.19', '- Repeat token ratio: adapter 0.06, base 0.04', '- Repeat 3-gram ratio: adapter 0.00, base 0.00', '- Length ok rate: adapter 0.00, base 0.10', '- Forbidden violation rate: adapter 0.00, base 0.00', '- CTA rate: adapter 1.00, base 0.80', '- Avg length: adapter 349.8, base 297.5', '', '## Per-sample Results', '', '| idx | persona | brand/product | stage | event | gpt winner | base len | adapter len | base cov | adapter cov | base tone | adapter tone | base style | adapter style | base dens | adapter dens | base rep tok | adapter rep tok | base rep 3g | adapter rep 3g | base len ok | adapter len ok | base forb | adapter forb | base cta | adapter cta |', '| --- | --- | --- | --- | --- | --- | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | --- | --- | ---: | ---: | --- | --- |', '', '## Examples', '']\n",
      "Saved report: /content/drive/MyDrive/멋사/comparison_dpo/comparison_01\n"
     ]
    }
   ],
   "source": [
    "_write_report(out_path, summary, results, args.max_examples)\n",
    "_log(f\"Saved report: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35It-4GT3Cyl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35It-4GT3Cyl",
    "outputId": "4510f0af-badd-49da-e2a3-811a9f8f0bbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'csv': '/content/AmoRe_crm_generator/finetuning/random_persona_campaign.csv',\n",
       " 'adapter': '/content/drive/MyDrive/멋사/adapters_dpo_2',\n",
       " 'samples': 10,\n",
       " 'metrics': ['GPT wins: adapter 5 / base 5 (skip_llm_eval=False)',\n",
       "  'Avg coverage: adapter 0.33, base 0.24',\n",
       "  'Avg tone match: adapter 0.00, base 0.00',\n",
       "  'Avg style match: adapter 0.05, base 0.07',\n",
       "  'Avg info density: adapter 0.15, base 0.19',\n",
       "  'Repeat token ratio: adapter 0.06, base 0.04',\n",
       "  'Repeat 3-gram ratio: adapter 0.00, base 0.00',\n",
       "  'Length ok rate: adapter 0.00, base 0.10',\n",
       "  'Forbidden violation rate: adapter 0.00, base 0.00',\n",
       "  'CTA rate: adapter 1.00, base 0.80',\n",
       "  'Avg length: adapter 349.8, base 297.5']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
