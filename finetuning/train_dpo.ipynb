{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qmrRdn2qWq_z",
      "metadata": {
        "id": "qmrRdn2qWq_z"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "코랩용 DPO (Direct Preference Optimization) 학습 스크립트\n",
        "./finetuning_data_dpo의 cycle_01.csv 파일을 토대로 1 사이클 DPO 학습 이후\n",
        "./checkpoints_dpo에 Trainer 등의 메타 데이터를 저장하고 이후 resume을 통해 추가 학습할 수 있도록 함.\n",
        "adapter의 경우 /content/drive/Mydrive/멋사/adapters/에 저장\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "df47cad1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df47cad1",
        "outputId": "9467ac36-9e59-4cba-835f-e69fe226f9f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "06a23c10",
      "metadata": {
        "id": "06a23c10",
        "outputId": "92ebe812-3e69-46c5-bfe3-9ddc902c9dc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Collecting trl\n",
            "  Downloading trl-0.26.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.57.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.7.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
            "Downloading trl-0.26.2-py3-none-any.whl (518 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes, trl\n",
            "Successfully installed bitsandbytes-0.49.0 trl-0.26.2\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Name: transformers\n",
            "Version: 4.57.3\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
            "Required-by: peft, sentence-transformers, trl\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets peft trl bitsandbytes accelerate\n",
        "!pip install -U transformers\n",
        "!pip show transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "iUhqoTSPL89O",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUhqoTSPL89O",
        "outputId": "93100b2b-e92d-486a-cee9-3f624c11ea39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0447caf8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0447caf8",
        "outputId": "8db51b63-fb6d-40dc-cdef-ae5b137d282b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "['.config', 'drive', '.env', '.ipynb_checkpoints', 'sample_data']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "dc31a301",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc31a301",
        "outputId": "4e676d46-3116-4513-aead-b3623ef41965"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AmoRe_crm_generator'...\n",
            "remote: Enumerating objects: 276, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 276 (delta 2), reused 5 (delta 2), pack-reused 262 (from 1)\u001b[K\n",
            "Receiving objects: 100% (276/276), 3.33 MiB | 7.33 MiB/s, done.\n",
            "Resolving deltas: 100% (157/157), done.\n",
            "/content/AmoRe_crm_generator\n",
            "Branch 'jinhyeok' set up to track remote branch 'jinhyeok' from 'origin'.\n",
            "Switched to a new branch 'jinhyeok'\n",
            "* \u001b[32mjinhyeok\u001b[m\n",
            "  main\u001b[m\n",
            "/content/AmoRe_crm_generator\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jjjh02/AmoRe_crm_generator.git\n",
        "%cd AmoRe_crm_generator\n",
        "!git checkout jinhyeok\n",
        "!git branch\n",
        "os.chdir(\"/content/AmoRe_crm_generator\")\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ],
      "metadata": {
        "id": "wTlqDHha0O9r",
        "outputId": "537ec348-fd7c-45be-fbd3-3c7270ae5dbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wTlqDHha0O9r",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fc959c1c",
      "metadata": {
        "id": "fc959c1c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "\n",
        "# 모델 및 경로 설정\n",
        "MODEL_ID = \"LGAI-EXAONE/EXAONE-4.0-1.2B\"\n",
        "CACHE_DIR = \"./models\"\n",
        "OUTPUT_DIR = \"./finetuning/checkpoints_dpo\"\n",
        "OUTPUT_ADAPTER_DIR = \"/content/drive/MyDrive/멋사/adapters_dpo_1_v2\"\n",
        "# BASE_ADAPTER_PATH = \"/content/drive/MyDrive/멋사/adapters_dpo_2\"\n",
        "NEW_ADAPTER_NAME = \"dpo_adapter_v2\"\n",
        "\n",
        "# 데이터셋 경로 설정\n",
        "DATA_DIR = \"/content/drive/MyDrive/멋사/dataset_dpo\"\n",
        "JSON_FILE = os.path.join(DATA_DIR, \"cycle_01_v2.json\")\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "PROMPT_LENGTH = 1024\n",
        "MAX_SEQ_LENGTH = 1512\n",
        "\n",
        "\n",
        "def load_dpo_dataset(json_path: str):\n",
        "    \"\"\"JSON 파일에서 DPO 형식의 데이터셋을 로드합니다.\n",
        "\n",
        "    JSON 형식:\n",
        "    [\n",
        "      { \"prompt\": \"...\", \"chosen\": \"...\", \"rejected\": \"...\" },\n",
        "      ...\n",
        "    ]\n",
        "\n",
        "    Args:\n",
        "        json_path: JSON 파일 경로\n",
        "\n",
        "    Returns:\n",
        "        train_dataset, eval_dataset\n",
        "    \"\"\"\n",
        "    # JSON 파일 로드\n",
        "    dataset = load_dataset(\n",
        "        \"json\",\n",
        "        data_files=json_path,\n",
        "    )\n",
        "    dataset = dataset[\"train\"]\n",
        "\n",
        "    # train / eval split\n",
        "    dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "    return dataset[\"train\"], dataset[\"test\"]\n",
        "\n",
        "\n",
        "def _freeze_all_params(model):\n",
        "    for _, param in model.named_parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "\n",
        "def _enable_adapter_params(model, adapter_name):\n",
        "    for name, param in model.named_parameters():\n",
        "        if f\".{adapter_name}.\" in name:\n",
        "            param.requires_grad = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ff1d730b",
      "metadata": {
        "id": "ff1d730b",
        "outputId": "1ac480fe-a2a8-4cfd-9789-828f5f893d38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1a3bbaf7f98743e995f7e1cfa8be8f95",
            "e57709010faa4ec0a5f212a7a5ec9364",
            "71dfde7cd9764734b81ef6456b31189f",
            "246730c9e1b04487b5d85988ea9b9a81",
            "b0be5ce588334f81803af9ac6e888dd4",
            "8c7c3f6e1dcc47a6a1c98d8699f87373",
            "adeed000a3f9438fba0d3bcb5e35aa55",
            "c77f791cfd204eb9a48e6efa37b12c57",
            "a485a97ce33649768baa2289838c6a67",
            "8c858c12530b4e16a5fc997cd820c12c",
            "0f15f5e54fb04875800604ec2ea6b138"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "토크나이저 로드 중...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file vocab.json from cache at ./models/models--LGAI-EXAONE--EXAONE-4.0-1.2B/snapshots/3abf2810673c7c0778df64a73c2d52eab32d91c4/vocab.json\n",
            "loading file merges.txt from cache at ./models/models--LGAI-EXAONE--EXAONE-4.0-1.2B/snapshots/3abf2810673c7c0778df64a73c2d52eab32d91c4/merges.txt\n",
            "loading file tokenizer.json from cache at ./models/models--LGAI-EXAONE--EXAONE-4.0-1.2B/snapshots/3abf2810673c7c0778df64a73c2d52eab32d91c4/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at ./models/models--LGAI-EXAONE--EXAONE-4.0-1.2B/snapshots/3abf2810673c7c0778df64a73c2d52eab32d91c4/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at ./models/models--LGAI-EXAONE--EXAONE-4.0-1.2B/snapshots/3abf2810673c7c0778df64a73c2d52eab32d91c4/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at ./models/models--LGAI-EXAONE--EXAONE-4.0-1.2B/snapshots/3abf2810673c7c0778df64a73c2d52eab32d91c4/chat_template.jinja\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터셋 로드 중: /content/drive/MyDrive/멋사/dataset_dpo/cycle_01_v2.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at ./models/models--LGAI-EXAONE--EXAONE-4.0-1.2B/snapshots/3abf2810673c7c0778df64a73c2d52eab32d91c4/config.json\n",
            "Model config Exaone4Config {\n",
            "  \"architectures\": [\n",
            "    \"Exaone4ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 361,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 65536,\n",
            "  \"model_type\": \"exaone4\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 30,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 16.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"sliding_window_pattern\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.57.3\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 102400\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at ./models/models--LGAI-EXAONE--EXAONE-4.0-1.2B/snapshots/3abf2810673c7c0778df64a73c2d52eab32d91c4/model.safetensors\n",
            "Instantiating Exaone4ForCausalLM model under default dtype torch.bfloat16.\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 361,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 데이터: 1123개, 평가 데이터: 125개\n",
            "모델 로드 중...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file generation_config.json from cache at ./models/models--LGAI-EXAONE--EXAONE-4.0-1.2B/snapshots/3abf2810673c7c0778df64a73c2d52eab32d91c4/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 361,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "Could not locate the custom_generate/generate.py inside LGAI-EXAONE/EXAONE-4.0-1.2B.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PEFT 설정 중...\n",
            "추가 어댑터 생성: dpo_adapter_v2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DPO Config 설정 중...\n",
            "DPOTrainer 초기화 중...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "Using auto half precision backend\n",
            "The following columns in the Training set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 1,123\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
            "  Gradient Accumulation steps = 3\n",
            "  Total optimization steps = 376\n",
            "  Number of trainable parameters = 60,948,480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 시작...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='376' max='376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [376/376 32:40, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rewards/chosen</th>\n",
              "      <th>Rewards/rejected</th>\n",
              "      <th>Rewards/accuracies</th>\n",
              "      <th>Rewards/margins</th>\n",
              "      <th>Logps/chosen</th>\n",
              "      <th>Logps/rejected</th>\n",
              "      <th>Logits/chosen</th>\n",
              "      <th>Logits/rejected</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.683299</td>\n",
              "      <td>-0.180104</td>\n",
              "      <td>-0.233855</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.053751</td>\n",
              "      <td>-684.730469</td>\n",
              "      <td>-741.759888</td>\n",
              "      <td>-3.709682</td>\n",
              "      <td>-3.639750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.608300</td>\n",
              "      <td>0.738490</td>\n",
              "      <td>-1.466380</td>\n",
              "      <td>-1.695951</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>0.229571</td>\n",
              "      <td>-697.593140</td>\n",
              "      <td>-756.380859</td>\n",
              "      <td>-3.727176</td>\n",
              "      <td>-3.657523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.467000</td>\n",
              "      <td>0.755930</td>\n",
              "      <td>-2.665429</td>\n",
              "      <td>-3.475004</td>\n",
              "      <td>0.648438</td>\n",
              "      <td>0.809575</td>\n",
              "      <td>-709.583618</td>\n",
              "      <td>-774.171326</td>\n",
              "      <td>-3.758888</td>\n",
              "      <td>-3.702438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.058700</td>\n",
              "      <td>0.661129</td>\n",
              "      <td>-2.616832</td>\n",
              "      <td>-4.502807</td>\n",
              "      <td>0.734375</td>\n",
              "      <td>1.885975</td>\n",
              "      <td>-709.097778</td>\n",
              "      <td>-784.449341</td>\n",
              "      <td>-3.748129</td>\n",
              "      <td>-3.681540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.706200</td>\n",
              "      <td>0.533892</td>\n",
              "      <td>-1.116477</td>\n",
              "      <td>-3.405584</td>\n",
              "      <td>0.781250</td>\n",
              "      <td>2.289107</td>\n",
              "      <td>-694.094177</td>\n",
              "      <td>-773.477112</td>\n",
              "      <td>-3.678073</td>\n",
              "      <td>-3.626885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.237100</td>\n",
              "      <td>0.421431</td>\n",
              "      <td>1.435554</td>\n",
              "      <td>-0.884238</td>\n",
              "      <td>0.820312</td>\n",
              "      <td>2.319792</td>\n",
              "      <td>-668.573914</td>\n",
              "      <td>-748.263672</td>\n",
              "      <td>-3.469806</td>\n",
              "      <td>-3.425312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.781000</td>\n",
              "      <td>0.410736</td>\n",
              "      <td>-1.538668</td>\n",
              "      <td>-4.835542</td>\n",
              "      <td>0.851562</td>\n",
              "      <td>3.296874</td>\n",
              "      <td>-698.316101</td>\n",
              "      <td>-787.776733</td>\n",
              "      <td>-3.622673</td>\n",
              "      <td>-3.592692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.555500</td>\n",
              "      <td>0.458349</td>\n",
              "      <td>1.179502</td>\n",
              "      <td>-1.581026</td>\n",
              "      <td>0.820312</td>\n",
              "      <td>2.760528</td>\n",
              "      <td>-671.134338</td>\n",
              "      <td>-755.231506</td>\n",
              "      <td>-3.563320</td>\n",
              "      <td>-3.526843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.227800</td>\n",
              "      <td>0.573121</td>\n",
              "      <td>-1.233341</td>\n",
              "      <td>-4.487390</td>\n",
              "      <td>0.789062</td>\n",
              "      <td>3.254049</td>\n",
              "      <td>-695.262817</td>\n",
              "      <td>-784.295166</td>\n",
              "      <td>-3.634358</td>\n",
              "      <td>-3.584069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.052000</td>\n",
              "      <td>0.563404</td>\n",
              "      <td>-3.884204</td>\n",
              "      <td>-8.463550</td>\n",
              "      <td>0.796875</td>\n",
              "      <td>4.579345</td>\n",
              "      <td>-721.771423</td>\n",
              "      <td>-824.056763</td>\n",
              "      <td>-3.992111</td>\n",
              "      <td>-3.964693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.113300</td>\n",
              "      <td>0.423553</td>\n",
              "      <td>-1.772368</td>\n",
              "      <td>-6.719733</td>\n",
              "      <td>0.835938</td>\n",
              "      <td>4.947364</td>\n",
              "      <td>-700.653076</td>\n",
              "      <td>-806.618591</td>\n",
              "      <td>-4.111589</td>\n",
              "      <td>-4.097209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.015100</td>\n",
              "      <td>0.385024</td>\n",
              "      <td>-0.929521</td>\n",
              "      <td>-6.282448</td>\n",
              "      <td>0.828125</td>\n",
              "      <td>5.352927</td>\n",
              "      <td>-692.224670</td>\n",
              "      <td>-802.245728</td>\n",
              "      <td>-4.182176</td>\n",
              "      <td>-4.176600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.023300</td>\n",
              "      <td>0.352966</td>\n",
              "      <td>0.220197</td>\n",
              "      <td>-4.995637</td>\n",
              "      <td>0.835938</td>\n",
              "      <td>5.215835</td>\n",
              "      <td>-680.727417</td>\n",
              "      <td>-789.377625</td>\n",
              "      <td>-4.002110</td>\n",
              "      <td>-3.986116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.002900</td>\n",
              "      <td>0.344758</td>\n",
              "      <td>1.995487</td>\n",
              "      <td>-3.304917</td>\n",
              "      <td>0.859375</td>\n",
              "      <td>5.300404</td>\n",
              "      <td>-662.974548</td>\n",
              "      <td>-772.470398</td>\n",
              "      <td>-3.808575</td>\n",
              "      <td>-3.782314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.006900</td>\n",
              "      <td>0.296580</td>\n",
              "      <td>1.738599</td>\n",
              "      <td>-4.504216</td>\n",
              "      <td>0.859375</td>\n",
              "      <td>6.242816</td>\n",
              "      <td>-665.543457</td>\n",
              "      <td>-784.463501</td>\n",
              "      <td>-3.836462</td>\n",
              "      <td>-3.821140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.029700</td>\n",
              "      <td>0.347384</td>\n",
              "      <td>-2.156764</td>\n",
              "      <td>-9.593256</td>\n",
              "      <td>0.867188</td>\n",
              "      <td>7.436492</td>\n",
              "      <td>-704.497070</td>\n",
              "      <td>-835.353882</td>\n",
              "      <td>-4.152637</td>\n",
              "      <td>-4.148557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.011900</td>\n",
              "      <td>0.419317</td>\n",
              "      <td>-5.419536</td>\n",
              "      <td>-13.740875</td>\n",
              "      <td>0.851562</td>\n",
              "      <td>8.321339</td>\n",
              "      <td>-737.124756</td>\n",
              "      <td>-876.830017</td>\n",
              "      <td>-4.236673</td>\n",
              "      <td>-4.228460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.004300</td>\n",
              "      <td>0.338847</td>\n",
              "      <td>-4.923725</td>\n",
              "      <td>-13.785478</td>\n",
              "      <td>0.882812</td>\n",
              "      <td>8.861753</td>\n",
              "      <td>-732.166626</td>\n",
              "      <td>-877.276062</td>\n",
              "      <td>-4.148728</td>\n",
              "      <td>-4.130573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.240743</td>\n",
              "      <td>-0.347290</td>\n",
              "      <td>-8.553138</td>\n",
              "      <td>0.898438</td>\n",
              "      <td>8.205847</td>\n",
              "      <td>-686.402283</td>\n",
              "      <td>-824.952576</td>\n",
              "      <td>-4.048156</td>\n",
              "      <td>-4.033998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.003300</td>\n",
              "      <td>0.183391</td>\n",
              "      <td>1.743248</td>\n",
              "      <td>-6.279818</td>\n",
              "      <td>0.914062</td>\n",
              "      <td>8.023064</td>\n",
              "      <td>-665.496948</td>\n",
              "      <td>-802.219421</td>\n",
              "      <td>-4.038784</td>\n",
              "      <td>-4.027321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.187260</td>\n",
              "      <td>0.490458</td>\n",
              "      <td>-8.466176</td>\n",
              "      <td>0.921875</td>\n",
              "      <td>8.956634</td>\n",
              "      <td>-678.024780</td>\n",
              "      <td>-824.083069</td>\n",
              "      <td>-4.151809</td>\n",
              "      <td>-4.143655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.182328</td>\n",
              "      <td>-1.534014</td>\n",
              "      <td>-11.240248</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>9.706235</td>\n",
              "      <td>-698.269531</td>\n",
              "      <td>-851.823730</td>\n",
              "      <td>-4.261046</td>\n",
              "      <td>-4.258357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>-3.072132</td>\n",
              "      <td>-13.417217</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>10.345085</td>\n",
              "      <td>-713.650757</td>\n",
              "      <td>-873.593445</td>\n",
              "      <td>-4.371731</td>\n",
              "      <td>-4.365901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.002300</td>\n",
              "      <td>0.253789</td>\n",
              "      <td>-3.581689</td>\n",
              "      <td>-14.015738</td>\n",
              "      <td>0.929688</td>\n",
              "      <td>10.434051</td>\n",
              "      <td>-718.746338</td>\n",
              "      <td>-879.578613</td>\n",
              "      <td>-4.389329</td>\n",
              "      <td>-4.391356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.007200</td>\n",
              "      <td>0.233393</td>\n",
              "      <td>-3.480060</td>\n",
              "      <td>-13.857805</td>\n",
              "      <td>0.929688</td>\n",
              "      <td>10.377745</td>\n",
              "      <td>-717.729980</td>\n",
              "      <td>-877.999329</td>\n",
              "      <td>-4.344624</td>\n",
              "      <td>-4.339168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.242964</td>\n",
              "      <td>-3.797458</td>\n",
              "      <td>-14.169413</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>10.371955</td>\n",
              "      <td>-720.903992</td>\n",
              "      <td>-881.115417</td>\n",
              "      <td>-4.317195</td>\n",
              "      <td>-4.314444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.241765</td>\n",
              "      <td>-3.809670</td>\n",
              "      <td>-14.181711</td>\n",
              "      <td>0.945312</td>\n",
              "      <td>10.372042</td>\n",
              "      <td>-721.026123</td>\n",
              "      <td>-881.238403</td>\n",
              "      <td>-4.301277</td>\n",
              "      <td>-4.298870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.228340</td>\n",
              "      <td>-2.561739</td>\n",
              "      <td>-12.719497</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>10.157760</td>\n",
              "      <td>-708.546814</td>\n",
              "      <td>-866.616272</td>\n",
              "      <td>-4.259882</td>\n",
              "      <td>-4.256005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.212857</td>\n",
              "      <td>-2.031329</td>\n",
              "      <td>-12.068485</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>10.037157</td>\n",
              "      <td>-703.242676</td>\n",
              "      <td>-860.106140</td>\n",
              "      <td>-4.238998</td>\n",
              "      <td>-4.232501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.210113</td>\n",
              "      <td>-1.997448</td>\n",
              "      <td>-12.014157</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>10.016709</td>\n",
              "      <td>-702.903931</td>\n",
              "      <td>-859.562927</td>\n",
              "      <td>-4.234256</td>\n",
              "      <td>-4.229665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.208972</td>\n",
              "      <td>-1.924953</td>\n",
              "      <td>-11.961786</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>10.036832</td>\n",
              "      <td>-702.179016</td>\n",
              "      <td>-859.039124</td>\n",
              "      <td>-4.234913</td>\n",
              "      <td>-4.229702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.218355</td>\n",
              "      <td>-1.981240</td>\n",
              "      <td>-11.972873</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>9.991632</td>\n",
              "      <td>-702.741821</td>\n",
              "      <td>-859.150024</td>\n",
              "      <td>-4.233999</td>\n",
              "      <td>-4.228540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.211187</td>\n",
              "      <td>-1.970037</td>\n",
              "      <td>-12.030542</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>10.060505</td>\n",
              "      <td>-702.629761</td>\n",
              "      <td>-859.726685</td>\n",
              "      <td>-4.232326</td>\n",
              "      <td>-4.228427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.218087</td>\n",
              "      <td>-1.972746</td>\n",
              "      <td>-11.969832</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>9.997087</td>\n",
              "      <td>-702.656860</td>\n",
              "      <td>-859.119629</td>\n",
              "      <td>-4.236032</td>\n",
              "      <td>-4.228996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.215230</td>\n",
              "      <td>-1.969423</td>\n",
              "      <td>-11.991642</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>10.022220</td>\n",
              "      <td>-702.623657</td>\n",
              "      <td>-859.337769</td>\n",
              "      <td>-4.234353</td>\n",
              "      <td>-4.228696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.219443</td>\n",
              "      <td>-1.994661</td>\n",
              "      <td>-12.019890</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>10.025228</td>\n",
              "      <td>-702.876038</td>\n",
              "      <td>-859.620117</td>\n",
              "      <td>-4.233747</td>\n",
              "      <td>-4.229004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.221371</td>\n",
              "      <td>-1.993428</td>\n",
              "      <td>-12.055747</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>10.062319</td>\n",
              "      <td>-702.863708</td>\n",
              "      <td>-859.978760</td>\n",
              "      <td>-4.234838</td>\n",
              "      <td>-4.229717</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "Saving model checkpoint to ./finetuning/checkpoints_dpo/checkpoint-100\n",
            "Configuration saved in ./finetuning/checkpoints_dpo/checkpoint-100/generation_config.json\n",
            "Detected adapters on the model, saving the model in the PEFT format, only adapter weights will be saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a3bbaf7f98743e995f7e1cfa8be8f95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--LGAI-EXAONE--EXAONE-4.0-1.2B/snapshots/3abf2810673c7c0778df64a73c2d52eab32d91c4/config.json\n",
            "Model config Exaone4Config {\n",
            "  \"architectures\": [\n",
            "    \"Exaone4ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 361,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 65536,\n",
            "  \"model_type\": \"exaone4\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 30,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 16.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"sliding_window_pattern\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.57.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 102400\n",
            "}\n",
            "\n",
            "To match the expected format of the PEFT library, all keys of the state dict of adapters will be prepended with `base_model.model`.\n",
            "Model weights saved in ./finetuning/checkpoints_dpo/checkpoint-100/adapter_model.safetensors\n",
            "chat template saved in ./finetuning/checkpoints_dpo/checkpoint-100/chat_template.jinja\n",
            "tokenizer config file saved in ./finetuning/checkpoints_dpo/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in ./finetuning/checkpoints_dpo/checkpoint-100/special_tokens_map.json\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "Saving model checkpoint to ./finetuning/checkpoints_dpo/checkpoint-200\n",
            "Configuration saved in ./finetuning/checkpoints_dpo/checkpoint-200/generation_config.json\n",
            "Detected adapters on the model, saving the model in the PEFT format, only adapter weights will be saved.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--LGAI-EXAONE--EXAONE-4.0-1.2B/snapshots/3abf2810673c7c0778df64a73c2d52eab32d91c4/config.json\n",
            "Model config Exaone4Config {\n",
            "  \"architectures\": [\n",
            "    \"Exaone4ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 361,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 65536,\n",
            "  \"model_type\": \"exaone4\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 30,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 16.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"sliding_window_pattern\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.57.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 102400\n",
            "}\n",
            "\n",
            "To match the expected format of the PEFT library, all keys of the state dict of adapters will be prepended with `base_model.model`.\n",
            "Model weights saved in ./finetuning/checkpoints_dpo/checkpoint-200/adapter_model.safetensors\n",
            "chat template saved in ./finetuning/checkpoints_dpo/checkpoint-200/chat_template.jinja\n",
            "tokenizer config file saved in ./finetuning/checkpoints_dpo/checkpoint-200/tokenizer_config.json\n",
            "Special tokens file saved in ./finetuning/checkpoints_dpo/checkpoint-200/special_tokens_map.json\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "Saving model checkpoint to ./finetuning/checkpoints_dpo/checkpoint-300\n",
            "Configuration saved in ./finetuning/checkpoints_dpo/checkpoint-300/generation_config.json\n",
            "Detected adapters on the model, saving the model in the PEFT format, only adapter weights will be saved.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--LGAI-EXAONE--EXAONE-4.0-1.2B/snapshots/3abf2810673c7c0778df64a73c2d52eab32d91c4/config.json\n",
            "Model config Exaone4Config {\n",
            "  \"architectures\": [\n",
            "    \"Exaone4ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 361,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 65536,\n",
            "  \"model_type\": \"exaone4\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 30,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 16.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"sliding_window_pattern\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.57.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 102400\n",
            "}\n",
            "\n",
            "To match the expected format of the PEFT library, all keys of the state dict of adapters will be prepended with `base_model.model`.\n",
            "Model weights saved in ./finetuning/checkpoints_dpo/checkpoint-300/adapter_model.safetensors\n",
            "chat template saved in ./finetuning/checkpoints_dpo/checkpoint-300/chat_template.jinja\n",
            "tokenizer config file saved in ./finetuning/checkpoints_dpo/checkpoint-300/tokenizer_config.json\n",
            "Special tokens file saved in ./finetuning/checkpoints_dpo/checkpoint-300/special_tokens_map.json\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `Exaone4ForCausalLM.forward` and have been ignored: reason_best, prompt, rejected_index, best_index, reason_rejected. If reason_best, prompt, rejected_index, best_index, reason_rejected are not expected by `Exaone4ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 125\n",
            "  Batch size = 4\n",
            "Saving model checkpoint to ./finetuning/checkpoints_dpo/checkpoint-376\n",
            "Configuration saved in ./finetuning/checkpoints_dpo/checkpoint-376/generation_config.json\n",
            "Detected adapters on the model, saving the model in the PEFT format, only adapter weights will be saved.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--LGAI-EXAONE--EXAONE-4.0-1.2B/snapshots/3abf2810673c7c0778df64a73c2d52eab32d91c4/config.json\n",
            "Model config Exaone4Config {\n",
            "  \"architectures\": [\n",
            "    \"Exaone4ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 361,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 65536,\n",
            "  \"model_type\": \"exaone4\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 30,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 16.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"sliding_window_pattern\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.57.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 102400\n",
            "}\n",
            "\n",
            "To match the expected format of the PEFT library, all keys of the state dict of adapters will be prepended with `base_model.model`.\n",
            "Model weights saved in ./finetuning/checkpoints_dpo/checkpoint-376/adapter_model.safetensors\n",
            "chat template saved in ./finetuning/checkpoints_dpo/checkpoint-376/chat_template.jinja\n",
            "tokenizer config file saved in ./finetuning/checkpoints_dpo/checkpoint-376/tokenizer_config.json\n",
            "Special tokens file saved in ./finetuning/checkpoints_dpo/checkpoint-376/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Saving model checkpoint to /content/drive/MyDrive/멋사/adapters_dpo_1_v2\n",
            "Configuration saved in /content/drive/MyDrive/멋사/adapters_dpo_1_v2/generation_config.json\n",
            "Detected adapters on the model, saving the model in the PEFT format, only adapter weights will be saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델 저장 중...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--LGAI-EXAONE--EXAONE-4.0-1.2B/snapshots/3abf2810673c7c0778df64a73c2d52eab32d91c4/config.json\n",
            "Model config Exaone4Config {\n",
            "  \"architectures\": [\n",
            "    \"Exaone4ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 361,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 65536,\n",
            "  \"model_type\": \"exaone4\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 30,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 16.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"sliding_window_pattern\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.57.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 102400\n",
            "}\n",
            "\n",
            "To match the expected format of the PEFT library, all keys of the state dict of adapters will be prepended with `base_model.model`.\n",
            "Model weights saved in /content/drive/MyDrive/멋사/adapters_dpo_1_v2/adapter_model.safetensors\n",
            "chat template saved in /content/drive/MyDrive/멋사/adapters_dpo_1_v2/chat_template.jinja\n",
            "tokenizer config file saved in /content/drive/MyDrive/멋사/adapters_dpo_1_v2/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/멋사/adapters_dpo_1_v2/special_tokens_map.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델이 저장되었습니다: /content/drive/MyDrive/멋사/adapters_dpo_1_v2\n"
          ]
        }
      ],
      "source": [
        "\"DPO 학습 메인 함수\"\n",
        "\n",
        "# 1. 토크나이저 로드\n",
        "print(\"토크나이저 로드 중...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    cache_dir=CACHE_DIR,\n",
        ")\n",
        "\n",
        "# pad_token 설정\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 패딩 사이드 설정 (DPO 학습에 유리)\n",
        "tokenizer.padding_side = 'left'\n",
        "tokenizer.truncation_side = 'left'\n",
        "\n",
        "# max_length 설정\n",
        "tokenizer.model_max_length = MAX_SEQ_LENGTH\n",
        "\n",
        "# 2. 데이터셋 로드\n",
        "print(f\"데이터셋 로드 중: {JSON_FILE}\")\n",
        "if not os.path.exists(JSON_FILE):\n",
        "    raise FileNotFoundError(f\"데이터셋 파일을 찾을 수 없습니다: {JSON_FILE}\")\n",
        "\n",
        "train_dataset, eval_dataset = load_dpo_dataset(JSON_FILE)\n",
        "print(f\"학습 데이터: {len(train_dataset)}개, 평가 데이터: {len(eval_dataset)}개\")\n",
        "\n",
        "# 3. Flash Attention 설정\n",
        "if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:\n",
        "    attn_implementation = \"flash_attention_2\"\n",
        "    torch_dtype = torch.bfloat16\n",
        "else:\n",
        "    attn_implementation = \"eager\"\n",
        "    torch_dtype = torch.float16\n",
        "\n",
        "# 4. 모델 로드\n",
        "print(\"모델 로드 중...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    use_cache=False,\n",
        "    # attn_implementation=attn_implementation,\n",
        "    torch_dtype=torch_dtype,\n",
        "    cache_dir=CACHE_DIR,\n",
        ")\n",
        "\n",
        "# 5. PEFT (LoRA) 설정\n",
        "print(\"PEFT 설정 중...\")\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.05,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# 6. 베이스 어댑터 로드 (학습하지 않음)\n",
        "# print(f\"베이스 어댑터 로드 중: {BASE_ADAPTER_PATH}\")\n",
        "# if not os.path.exists(BASE_ADAPTER_PATH):\n",
        "#     raise FileNotFoundError(f\"베이스 어댑터를 찾을 수 없습니다: {BASE_ADAPTER_PATH}\")\n",
        "\n",
        "# model = PeftModel.from_pretrained(\n",
        "#     model,\n",
        "#     BASE_ADAPTER_PATH,\n",
        "#     is_trainable=False,\n",
        "# )\n",
        "\n",
        "# 7. 추가 어댑터 생성 및 활성화\n",
        "print(f\"추가 어댑터 생성: {NEW_ADAPTER_NAME}\")\n",
        "model.add_adapter(peft_config, NEW_ADAPTER_NAME)\n",
        "model.set_adapter(NEW_ADAPTER_NAME)\n",
        "_freeze_all_params(model)\n",
        "_enable_adapter_params(model, NEW_ADAPTER_NAME)\n",
        "\n",
        "# 8. DPO Config 설정\n",
        "print(\"DPO Config 설정 중...\")\n",
        "dpo_config = DPOConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=3,\n",
        "    learning_rate=5e-5,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_steps=1,\n",
        "    logging_first_step=True,\n",
        "    logging_strategy=\"steps\",\n",
        "    log_level=\"info\",\n",
        "    disable_tqdm=False,\n",
        "    save_steps=100,\n",
        "    save_total_limit=20,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=10,\n",
        "    # fp16=True,\n",
        "    beta=0.1,\n",
        "    loss_type=\"sigmoid\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# 9. DPOTrainer 초기화\n",
        "print(\"DPOTrainer 초기화 중...\")\n",
        "trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    ref_model=None,  # PEFT 사용 시 None으로 설정\n",
        "    args=dpo_config,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "# 10. 학습 시작\n",
        "print(\"학습 시작...\")\n",
        "ckpt_dir = \"AmoRe_crm_generator/finetuning/checkpoints_dpo\"\n",
        "\n",
        "resume = None\n",
        "if os.path.isdir(ckpt_dir) and len(os.listdir(ckpt_dir)) > 0:\n",
        "    resume = True\n",
        "\n",
        "trainer.train(resume_from_checkpoint=resume)\n",
        "\n",
        "# 11. 모델 저장\n",
        "print(\"모델 저장 중...\")\n",
        "trainer.save_model(OUTPUT_ADAPTER_DIR)\n",
        "print(f\"모델이 저장되었습니다: {OUTPUT_ADAPTER_DIR}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZON9ITbA3wVX"
      },
      "id": "ZON9ITbA3wVX",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1a3bbaf7f98743e995f7e1cfa8be8f95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e57709010faa4ec0a5f212a7a5ec9364",
              "IPY_MODEL_71dfde7cd9764734b81ef6456b31189f",
              "IPY_MODEL_246730c9e1b04487b5d85988ea9b9a81"
            ],
            "layout": "IPY_MODEL_b0be5ce588334f81803af9ac6e888dd4"
          }
        },
        "e57709010faa4ec0a5f212a7a5ec9364": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c7c3f6e1dcc47a6a1c98d8699f87373",
            "placeholder": "​",
            "style": "IPY_MODEL_adeed000a3f9438fba0d3bcb5e35aa55",
            "value": "config.json: "
          }
        },
        "71dfde7cd9764734b81ef6456b31189f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c77f791cfd204eb9a48e6efa37b12c57",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a485a97ce33649768baa2289838c6a67",
            "value": 1
          }
        },
        "246730c9e1b04487b5d85988ea9b9a81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c858c12530b4e16a5fc997cd820c12c",
            "placeholder": "​",
            "style": "IPY_MODEL_0f15f5e54fb04875800604ec2ea6b138",
            "value": " 1.53k/? [00:00&lt;00:00, 179kB/s]"
          }
        },
        "b0be5ce588334f81803af9ac6e888dd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c7c3f6e1dcc47a6a1c98d8699f87373": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adeed000a3f9438fba0d3bcb5e35aa55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c77f791cfd204eb9a48e6efa37b12c57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a485a97ce33649768baa2289838c6a67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c858c12530b4e16a5fc997cd820c12c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f15f5e54fb04875800604ec2ea6b138": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}